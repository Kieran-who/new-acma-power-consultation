{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "\n",
    "In this step we carry out the following:\n",
    "\n",
    "1. We load the spreadsheet from Sharepoint which includes the list of\n",
    "   submissions, the human defined categories for submitters of interest and any\n",
    "   further annotations such as whether we are removing submissions from\n",
    "   analysis. We format this data into a json file, saved in\n",
    "   `./data/step1/list.json`. This JSON file allows for easier manipulation and\n",
    "   handling.\n",
    "2. Using the JSON file from step 1, we create a number of jsonl files that are\n",
    "   in the correct format for processing by OpenAI's batch API. We create\n",
    "   multiple jsonl files as each has to be less than 100mb in size.\n",
    "3. We upload the jsonl files, and trigger the batch processing of them. This can\n",
    "   take upwards of 24 hours.\n",
    "4. Once processing is complete, we download the completed responses for each\n",
    "   request, and update the json file from step 1 to include the AI returned\n",
    "   data. We also export this data in a spreadsheet for review\n",
    "   (`./data/step1/review1.xlsx`)\n",
    "\n",
    "After this step, not only do we have preliminary data for all submissions\n",
    "(answers to questions below), but we can also check if the AI has catagorised\n",
    "any of the unlabeled submissions (that we may have missed) into the categories\n",
    "of interest. Once we are settled on the categories, we can move onto `Step 2`\n",
    "which involves asking the AI the specific questions for each category.\n",
    "\n",
    "This step asks the AI to evaluate each submission based on the following\n",
    "questions:\n",
    "\n",
    "- **substantive_submission**: Does the submission provide a substantive response\n",
    "  to the consultation (True or False)? Substantive submissions are those that\n",
    "  provide a detailed or well reasoned response to the consultation. Screenshots\n",
    "  of memes or other pre-existing content are not considered substantive.\n",
    "  Submissions relying on conspiracy theories are not considered substantive. A\n",
    "  submission expressing purely personal opinion, without any supporting\n",
    "  argument, is not considered substantive. Consider the supplementary materials\n",
    "  and determine if the submission substantively answers the issues, concerns and\n",
    "  questions raised in the consultation.\n",
    "\n",
    "- **responder_category**: One aspect of the research is looking how different\n",
    "  categories of responders respond to the consultation. Based on the response\n",
    "  (especially the name of the responder - e.g. if it is a company or group),\n",
    "  please select the category that best describes the responder. Options are: 1.\n",
    "  Individual, 2. Political (e.g. politician or political part), 3. Digital\n",
    "  Platform (e.g. Meta, Microsoft, Google, etc.), 4. Civil Society (e.g. NGO,\n",
    "  advocacy group, etc.), 5. Academic, 6. News (e.g. a news company such as News\n",
    "  Corp, ABC or Nine News, or an industry association representing news\n",
    "  organisations such as Australian Press Council, Commerical Radio Australia or\n",
    "  FreeTV), 7. Government (e.g. government agencies such as Victorian Electoral\n",
    "  Commission or Australian Human Rights Commission), 8. Industry (An industry\n",
    "  body that does not neatly fit within the predefined categories (e.g. while\n",
    "  Commerical Radio Australia represents news broadcasters and as such fits\n",
    "  within News, an industry body such as Communications Alliance, which\n",
    "  represents communications providers such as a telcos and broadband companies\n",
    "  would fit here)), 9. Other (please specify). Only return the category that\n",
    "  best describes the responder. E.g. for a submission from the UTS Centre for\n",
    "  Media Transition, you would return: 'Academic'.\n",
    "\n",
    "- **support**: Overall, considering the whole of the submission, does the\n",
    "  submission support, oppose, or have a neutral stance towards the proposed\n",
    "  laws? Make sure you truly understand the submission's position taking into\n",
    "  account the whole document. Look for express statement's expressing support or\n",
    "  opposition. If no express statements are present, weigh up the arguments\n",
    "  against and arguments in favour of the Bill and specific aspects of the\n",
    "  proposed changes. Some confusion may arise where the submission states it is\n",
    "  supporting another submission â€“ this does not mean the submission is in\n",
    "  support of the proposed changes. In these circumstances, if you are unsure\n",
    "  (i.e. the submission is not clear and you do not have access to the submission\n",
    "  being referred to), respond with 'unsure'. Return only one of the following\n",
    "  options: 'support', 'oppose', 'neutral', 'unsure'.\n",
    "\n",
    "- **motivations**: Number the top 3 motivations or concerns underpinning the\n",
    "  submission's viewpoint. Keep each motivation general and brief (under five\n",
    "  words). If there are only one or two key motivations in the submission, return\n",
    "  only these one or two.\n",
    "\n",
    "- **changes**: Does this submission provide suggestions on what changes need to\n",
    "  be made to the Bill? If so, please list each suggested change with a very\n",
    "  brief summary (no more than a line in total), with the most important changes\n",
    "  first. You do not need to describe each change in detail; just give a one-line\n",
    "  statement of what the submitter requests to be changed. If there is no comment\n",
    "  on this aspect, return 'No comment'..\n",
    "\n",
    "- **regulation**: Does this submission make any comment on the form of\n",
    "  regulation provided by the Bill? For example, does the submission comment on\n",
    "  the practicality, feasibility or merits of self-regulation, codes of practice,\n",
    "  industry standards or direct government regulation (i.e. in the form of direct\n",
    "  legislation)? This question is only interested in the submission's view on the\n",
    "  FORM OF REGULATION and NOT whether or not the issue should be regulated. It is\n",
    "  also not interested in the submission's view as to what the impacts of\n",
    "  regulation may be. We are also not interested in the submission's view of the\n",
    "  impacts that regulation, in general, will have. WE ARE ONLY interested in\n",
    "  specific comments as to the form of regulation (e.g. self-regulation,\n",
    "  quasi-regulation, co-regulation, industry codes of practice,\n",
    "  industry-regulator collaboration, industry standards, Direct government\n",
    "  regulation, etc.). If the submission makes explicit comments as to the form of\n",
    "  regulation, please provide a brief summary of the comments and the reasoning\n",
    "  behind their belief. If there is no comment on this aspect, return 'No\n",
    "  comment'.\n",
    "\n",
    "- **perceived_societal_impact**: How does the submitter perceive the impact of\n",
    "  the proposed laws on combating misinformation and disinformation and the\n",
    "  broader digital ecosystem, including social media platforms, content creators,\n",
    "  and the general public? Restrict your summary on this point purely to what\n",
    "  impacts the submission has highlighted relating to combating misinformation\n",
    "  and disinformation and the broader digital ecosystem, including social media\n",
    "  platforms, content creators, and the general public. Do not include comments\n",
    "  on any aspects covered by other questions. If there is no comment on this\n",
    "  aspect, return 'No comment'.\n",
    "\n",
    "- **regulator_trust**: Does the submission express trust or scepticism towards\n",
    "  the Australian Media and Communications Authority (ACMA)'s ability to\n",
    "  impartially and effectively use the new powers? Are there any suggestions in\n",
    "  the submission for ensuring accountability and oversight? Please only\n",
    "  highlight express statements as to the ability of the ACMA to impartially and\n",
    "  effectively use the new powers or any express suggestions for ensuring\n",
    "  accountability and oversight of the ACMA exercising its power. If there is no\n",
    "  comment on this aspect, return 'No comment'.\n",
    "\n",
    "- **definitions**: What does the submitter think about the definitions of\n",
    "  'misinformation', 'disinformation' and 'serious harm'? Only consider specific\n",
    "  comments on the definitions of 'misinformation', 'disinformation' and 'serious\n",
    "  harm'. If the submitter makes no comment on the definitions of these specific\n",
    "  terms, return 'No comment'.\n",
    "\n",
    "Each call to the AI is formatted with the below prompt. The supplementary\n",
    "materials,\n",
    "[the online page](https://www.infrastructure.gov.au/have-your-say/new-acma-powers-combat-misinformation-and-disinformation)\n",
    "and\n",
    "[fact sheet](https://www.infrastructure.gov.au/department/media/publications/communications-legislation-amendment-combatting-misinformation-and-disinformation-bill-2023-fact)\n",
    "provided as part of the consultation are inserted during the process (replacing\n",
    "the placeholders: `|issues|` & `|fact_sheet|`). `submission_eval` is a reference\n",
    "to the formatted questions above which are provided as a `tool` (a `tool` just\n",
    "ensures responses are in the correct format). The submission is added, clearly\n",
    "noted as such, to the base of the prompt (as per function `prompt_formatted` in\n",
    "step 2.).\n",
    "\n",
    "---\n",
    "\n",
    "```text\n",
    "LLM is a highly paid professor who is world-renowned for their expertise in the regulation of misinformation and disinformation in Australia.\n",
    "\n",
    "LLM is working on a research project looking into different attitudes to the Australian Government's proposed laws to provide the independent regulator, the Australian Communications and Media Authority (ACMA), with new powers to combat online misinformation and disinformation.\n",
    "\n",
    "LLM's first task is to carefully read and understand the supplementary material. The supplementary material will be provided below. It is information provided by the Australian Government in its consultation process to gather input from citizens. Each document will be numbered as SUPPLEMENTARY DOCUMENT 1. SUPPLEMENTARY DOCUMENT 2. etc. A small note introducing each supplementary document will be provided.\n",
    "\n",
    "Once LLM has understood the supplementary material, LLM will then move on to a public submission in response to the proposed new powers. LLM must carefully read this submission. The start and end of the submission will be clearly labelled. Once LLM has read and understood the submission, LLM must answer the questions noted in the function `submission_eval` and return its response in valid JSON format. Each question in the function definition should first be read so that LLM knows not to repeat itself in different questions. The response to each function argument must be carefully considered. Before returning its response, LLM must first formulate its response, reconsider if the response answers the question to the quality expected of a world-renowed expert on the matter and reformulate the response if necessary to meet these quality expectations. Only after following this process should LLM return its response to each function argument.\n",
    "\n",
    "LLM is working on this task as part of a research group. LLM's role in carrying out this task to the highest standard of academic excellence is vital in achieving the project goals and will ensure LLM and the rest of the project group not only make a large positive societal impact but will significantly advance LLM and each member of the research group's academic careers.\n",
    "\n",
    "############################## SUPPLEMENTARY MATERIALS ##############################\n",
    "\n",
    "SUPPLEMENTARY DOCUMENT 1.\n",
    "\n",
    "Description: This document is the text provided on the public website that the Australian Government set up to provide information to citizens to inform their submissions.\n",
    "\n",
    "-------------------------------- SUPPLEMENTARY DOCUMENT 1. START --------------------------------\n",
    "\n",
    "|issues|\n",
    "\n",
    "-------------------------------- SUPPLEMENTARY DOCUMENT 1. END --------------------------------\n",
    "\n",
    "SUPPLEMENTARY DOCUMENT 2.\n",
    "\n",
    "Description: This document is a fact sheet provided by the Australian Government. It briefly explains some of the key elements of the proposed law changes and the issues they want input on.\n",
    "\n",
    "-------------------------------- SUPPLEMENTARY DOCUMENT 2. START --------------------------------\n",
    "\n",
    "|fact_sheet|\n",
    "\n",
    "-------------------------------- SUPPLEMENTARY DOCUMENT 2. END --------------------------------\n",
    "\n",
    "############################## SUPPLEMENTARY MATERIALS END ##############################\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading spreadsheet\n",
    "\n",
    "[The spreadsheet](https://studentutsedu.sharepoint.com/:x:/r/sites/CentreforMediaTransition76/_layouts/15/doc2.aspx?sourcedoc=%7B26015E46-DC17-46F8-85CB-8FF7601BB93E%7D&file=List%20of%20all%20submissions.xlsx&action=default&mobileredirect=true&DefaultItemOpen=1&ct=1715733887026&wdOrigin=OFFICECOM-WEB.START.REC&cid=c80abce9-d7d3-419b-8b78-2504ae4ce71a&wdPreviousSessionSrc=HarmonyWeb&wdPreviousSession=a8d0354e-a231-454b-a8a5-18124a5f1983)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 submissions missing files: []\n",
      "9 submissions using manually extracted files: ['14193', '18110', '26222', '34756', 'e656', '34418', '33824', 'e597', '19712']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Load the Excel spreadsheet into a pandas DataFrame\n",
    "df = pd.read_excel('./data/step1/list.xlsx')\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    parts = filename.split('-')\n",
    "    name_parts = parts[1:]\n",
    "    name = ' '.join(name_parts).split('.')[0]\n",
    "    if name.find('anonymous') != -1:\n",
    "        name = 'anonymous'\n",
    "    return name.lower()\n",
    "\n",
    "missing_files = []\n",
    "manual_files = []\n",
    "\n",
    "list_of_files_manually_text_extracted = ['e656', '14193', '18110', '19712', '26222', '33824', '34418', '34756', 'e597' ]\n",
    "\n",
    "def check_file_exists(doc_id, folder_path = './data/files'):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(doc_id) and doc_id not in list_of_files_manually_text_extracted:\n",
    "            return True\n",
    "    if doc_id not in list_of_files_manually_text_extracted:\n",
    "        missing_files.append(doc_id)\n",
    "        return False\n",
    "    manual_files.append(doc_id)\n",
    "    return False\n",
    "\n",
    "formatted_data = []\n",
    "# Convert empty cells in 'Group', 'Comments', and 'Removed (Y)' columns to None\n",
    "for row in data:\n",
    "    if pd.isnull(row['Group']):\n",
    "        row['Group'] = None\n",
    "    if pd.isnull(row['Comments']):\n",
    "        row['Comments'] = None\n",
    "    if pd.isnull(row['Removed (Y)']):\n",
    "        row['Removed (Y)'] = None\n",
    "\n",
    "    exist = check_file_exists(row['UniqueID'])\n",
    "    \n",
    "    formatted_row = {\n",
    "        'uniqueId': row['UniqueID'],\n",
    "        'group': row['Group'],\n",
    "        'submitter': extract_name_from_filename(row['doc']),        \n",
    "        \"metadata\": {\n",
    "            \"groupDefinedBy\": \"human\" if row['Group'] else \"AI\",\n",
    "            \"removed\": row['Removed (Y)'],\n",
    "            \"comments\": row['Comments'],\n",
    "            \"text_extraction_method\": \"Marker2\" if exist else 'manual'\n",
    "        }\n",
    "    }\n",
    "    formatted_data.append(formatted_row)\n",
    "    \n",
    "# Save the data as a JSON file if it doesn't exist\n",
    "json_file = './data/step1/list.json'\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')  # Adjust to your local timezone\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "output = {\n",
    "    \"data\": formatted_data,\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": current_time,\n",
    "        \"submissions_missing_files\": missing_files,\n",
    "        \"submissions_using_manually_extracted_files\": manual_files\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(output, f)\n",
    "\n",
    "print(F'{len(missing_files)} submissions missing files: {missing_files}')\n",
    "print(F'{len(manual_files)} submissions using manually extracted files: {manual_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. We now have a JSON file of objects with key value pairs in the below form. We now process this to jsonl form for batch processing\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"group\": \"string | null\",\n",
    "  \"submitter\": \"string\",\n",
    "  \"uniqueId\": \"string\",\n",
    "  \"metadata\": {\n",
    "    \"groupDefinedBy\": \"human or AI\",\n",
    "    \"removed\": \"string | null\",\n",
    "    \"comments\": \"string | null\",\n",
    "    \"text_extraction_method\": \"Marker2 | manual\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We ignore in this step any submissions we flag as removed.\n",
    "\n",
    "jsonl files saved to `./data/step1/toProcess`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'), max_retries=3)\n",
    "\n",
    "def parse_JSON(json_str: str) -> dict:        \n",
    "    try: \n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:              \n",
    "        messages = [\n",
    "      {\n",
    "        'role': 'system',\n",
    "        'content':\n",
    "          'Assistant is a large language model designed to fix and return correct JSON objects.',\n",
    "      },\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': f'ORIGINAL ERROR CONTAINING JSON OBJECT:\\n\\n{json_str}\\n\\nERROR MESSAGE: {e}',\n",
    "      },\n",
    "    ]\n",
    "        \n",
    "        tool_choices = [{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'fix_object',\n",
    "        'description':\n",
    "          'You will be given an incorrectly formed JSON Object and a error message. You must fix the incorrect JSON Object and return the valid JSON object.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'fixedJSON': {\n",
    "              'type': 'string',\n",
    "              'description': 'The reformated and error free JSON object. Return the JSON object only!',\n",
    "            },\n",
    "          },\n",
    "          'required': ['fixedJSON'],\n",
    "        },\n",
    "      },\n",
    "    }]                \n",
    "        additional_params = {\n",
    "          'messages': messages,\n",
    "          'tools': tool_choices,\n",
    "          'temperature': 0,\n",
    "          'max_tokens': 4096,\n",
    "          'tool_choice':{ 'type': 'function', 'function': { 'name': 'fix_object' } }\n",
    "          }\n",
    "        params = {**additional_params} \n",
    "        response = client.chat.completions.create(params)\n",
    "                \n",
    "        second_test_json = response.choices[0].message.tool_calls[0].function.arguments \n",
    "                  \n",
    "        to_return = json.loads(second_test_json)\n",
    "        return json.loads(to_return['fixedJSON'])\n",
    "\n",
    "promt_file = 'prompt_no_guidance.txt'\n",
    "# Define the prompt for each individual request\n",
    "def prompt_formatted() -> str:    \n",
    "    # Read the first file and set a string variable\n",
    "    with open(promt_file, 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    with open('prompt_issues.md', 'r') as file:\n",
    "        issues = file.read()\n",
    "\n",
    "    # with open('prompt_guidance_note.md', 'r') as file:\n",
    "    #     guidance_note = file.read()\n",
    "\n",
    "    with open('prompt_fact_sheet.md', 'r') as file:\n",
    "        fact_sheet = file.read()\n",
    "\n",
    "    prompt = prompt.replace('|issues|', issues)\n",
    "    # prompt = prompt.replace('|guidance_note|', guidance_note)\n",
    "    prompt = prompt.replace('|fact_sheet|', fact_sheet)    \n",
    "\n",
    "    return prompt\n",
    "\n",
    "def get_submission(submission_text: str, submission_author: str):\n",
    "    prompt = \"\"\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION START *****************************************\\n\\n\"\n",
    "\n",
    "    prompt += f\"Submission from: {submission_author}\\n\\n\"\n",
    "    \n",
    "    prompt += submission_text\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION END *****************************************\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def get_function():\n",
    "    with open('function.json', 'r') as f:\n",
    "        function = json.load(f)\n",
    "    return function\n",
    "\n",
    "def get_file_path(doc_id, folder_path = './data/files'):    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(doc_id):\n",
    "            return os.path.join(folder_path, file_name)\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "md_file_location = './data/files'\n",
    "\n",
    "file_counter = 0\n",
    "jsonl_file = f\"./data/step1/toProcess/jsonl_{file_counter}.jsonl\"\n",
    "\n",
    "skipped_files = []\n",
    "empty_files = []\n",
    "\n",
    "os.makedirs('./data/step1/toProcess', exist_ok=True)\n",
    "\n",
    "responses = []\n",
    "# This loop takes each submission and adds it to the jsonl file in a format that can be used by the OpenAI API\n",
    "for i in list[\"data\"][:5]:      \n",
    "    md_file_path = get_file_path(i.get(\"uniqueId\"))       \n",
    "    with open(md_file_path, 'r') as file:\n",
    "        submission = file.read()        \n",
    "    if len(submission.strip()) == 0:\n",
    "        continue\n",
    "    sub_author = i[\"submitter\"]\n",
    "    prompt = prompt_formatted()\n",
    "    submission_formatted = get_submission(submission, sub_author)\n",
    "\n",
    "    function = get_function()\n",
    "    ldata = {\"custom_id\": i[\"uniqueId\"], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": submission_formatted}], \"max_tokens\": 4096, \"temperature\": 1e-9, \"frequency_penalty\": 0, \"presence_penalty\" :0, \"top_p\":0, \"tools\":[function], \"tool_choice\": { \"type\": \"function\", \"function\": { \"name\": \"submission_eval\" } }}}\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model='gpt-4o-2024-05-13',\n",
    "                    messages=[{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": submission_formatted}],\n",
    "                    max_tokens=4096,\n",
    "                    temperature=1e-9,\n",
    "                    tools=[function],\n",
    "                    tool_choice={ 'type': 'function', 'function': { 'name': 'submission_eval' } },\n",
    "                    frequency_penalty=0,\n",
    "                    presence_penalty=0,\n",
    "                    top_p=0\n",
    "                )\n",
    "    json_res = parse_JSON(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "    responses.append({i['uniqueId']: json_res, \"meta\": {\"custom_id\": i[\"uniqueId\"], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": submission_formatted}], \"max_tokens\": 4096, \"temperature\": 1e-9, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"top_p\": 0, \"tools\":[function], \"tool_choice\": { \"type\": \"function\", \"function\": { \"name\": \"submission_eval\" } }}}})\n",
    "\n",
    "with open('./data/test.json', 'w') as f:\n",
    "    json.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty files: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "promt_file = 'prompt_no_guidance.txt'\n",
    "# Define the prompt for each individual request\n",
    "def prompt_formatted(submission_string: str, submission_author: str) -> str:    \n",
    "    # Read the first file and set a string variable\n",
    "    with open(promt_file, 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    with open('prompt_issues.md', 'r') as file:\n",
    "        issues = file.read()\n",
    "\n",
    "    # with open('prompt_guidance_note.md', 'r') as file:\n",
    "    #     guidance_note = file.read()\n",
    "\n",
    "    with open('prompt_fact_sheet.md', 'r') as file:\n",
    "        fact_sheet = file.read()\n",
    "\n",
    "    prompt = prompt.replace('|issues|', issues)\n",
    "    # prompt = prompt.replace('|guidance_note|', guidance_note)\n",
    "    prompt = prompt.replace('|fact_sheet|', fact_sheet)\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION START *****************************************\\n\\n\"\n",
    "\n",
    "    prompt += f\"Submission from: {submission_author}\\n\\n\"\n",
    "    \n",
    "    prompt += submission_string\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION END *****************************************\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def get_function():\n",
    "    with open('function.json', 'r') as f:\n",
    "        function = json.load(f)\n",
    "    return function\n",
    "\n",
    "def get_file_path(doc_id, folder_path = './data/files'):    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(doc_id):\n",
    "            return os.path.join(folder_path, file_name)\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "md_file_location = './data/files'\n",
    "\n",
    "file_counter = 0\n",
    "jsonl_file = f\"./data/step1/toProcess/jsonl_{file_counter}.jsonl\"\n",
    "\n",
    "skipped_files = []\n",
    "empty_files = []\n",
    "\n",
    "os.makedirs('./data/step1/toProcess', exist_ok=True)\n",
    "\n",
    "# This loop takes each submission and adds it to the jsonl file in a format that can be used by the OpenAI API\n",
    "for i in list[\"data\"]:   \n",
    "    if i[\"metadata\"][\"removed\"] == \"Y\":\n",
    "        skipped_files.append(i[\"uniqueId\"])\n",
    "        continue\n",
    "    try:\n",
    "        md_file_path = get_file_path(i.get(\"uniqueId\"))       \n",
    "        with open(md_file_path, 'r') as file:\n",
    "            submission = file.read()        \n",
    "        if len(submission.strip()) == 0:\n",
    "            i[\"metadata\"][\"removed\"] = \"Y\"\n",
    "            i[\"metadata\"][\"comments\"] = f\"{i[\"metadata\"][\"comments\"]}\\n\\nRemoved due to empty file\"\n",
    "            empty_files.append(i[\"uniqueId\"])\n",
    "            continue\n",
    "        sub_author = i[\"submitter\"]\n",
    "        prompt = prompt_formatted(submission, sub_author)\n",
    "        function = get_function()\n",
    "        ldata = {\"custom_id\": i[\"uniqueId\"], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"max_tokens\": 4096, \"temperature\": 1e-9, \"frequency_penalty\": 0, \"presence_penalty\" :0, \"top_p\":0, \"tools\":[function], \"tool_choice\": { \"type\": \"function\", \"function\": { \"name\": \"submission_eval\" } }}}\n",
    "\n",
    "        i[\"metadata\"][\"ai_parameters\"] = {\"custom_id\": i[\"uniqueId\"], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"max_tokens\": 4096, \"temperature\": 1e-9, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"top_p\": 0, \"tools\":[function], \"tool_choice\": { \"type\": \"function\", \"function\": { \"name\": \"submission_eval\" } }}}\n",
    "        \n",
    "        if os.path.exists(jsonl_file) and os.path.getsize(jsonl_file) >= 0.5 * 1024 * 1024:  # 0.5MB (very small files as current restrictions on OpneAI API due to low tier)\n",
    "            file_counter += 1\n",
    "            jsonl_file = f\"./data/step1/toProcess/jsonl_{file_counter}.jsonl\"\n",
    "        \n",
    "        i[\"metadata\"][\"batch\"] = f'jsonl_{file_counter}.jsonl'\n",
    "        \n",
    "        with open(jsonl_file, 'a') as f:\n",
    "            json.dump(ldata, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list, f)\n",
    "\n",
    "print(f\"Empty files: {empty_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We now have a folder with all the prepared files for OpenAI batch calls\n",
    "\n",
    "We will upload each of these files to OpenAI and then trigger batch processing\n",
    "of each.\n",
    "\n",
    "**MAKE SURE TO RECORD BATCH IDs CREATED IN THIS STEP SO WE KNOW WHICH FILES TO\n",
    "EVENTUALLY DOWNLOAD**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'), max_retries=3)\n",
    "\n",
    "jsonl_dir = './data/step1/toProcess'\n",
    "\n",
    "jsonl_files = [f for f in os.listdir(jsonl_dir) if os.path.isfile(os.path.join(jsonl_dir, f)) and f.endswith('.jsonl')]\n",
    "\n",
    "file_ids = []\n",
    "\n",
    "for file in jsonl_files:\n",
    "    file_object = client.files.create(\n",
    "        file=open(f\"{jsonl_dir}/{file}\", \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    file_ids.append(file_object.id)\n",
    "\n",
    "print(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record the following and make sure to add to `desired_batch_ids` in the following cells!\n",
      "['batch_chdZHGDMh0lz2CSM4bsQ960a']\n"
     ]
    }
   ],
   "source": [
    "# We have now uploaded all the files and have their IDs, lets create a batch job for each\n",
    "batch_ids = []\n",
    "\n",
    "for file_id in file_ids:\n",
    "    job = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "          )\n",
    "    batch_ids.append(job.id)\n",
    "\n",
    "print('Record the following and make sure to add to `desired_batch_ids` in the following cells!')\n",
    "print(batch_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The batch processes should now be underway, they will take up to 24hrs\n",
    "\n",
    "We can run the following cell to check on process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "batch_jobs = client.batches.list()\n",
    "\n",
    "desired_batch_ids = ['', '']\n",
    "\n",
    "for batch in batch_jobs.data:\n",
    "    if batch.id in desired_batch_ids:\n",
    "        print(batch.id, batch.status, batch.request_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once processing is done, we can download the completed files\n",
    "\n",
    "Files are saved here: `./data/step1/output`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "batch_jobs = client.batches.list()\n",
    "\n",
    "# we only want to download the batch jobs that were set up in cell 11\n",
    "desired_batch_ids = ['batch_vdZSGRcPyfdMH8T0UCfssPpw']\n",
    "\n",
    "for batch in batch_jobs.data:\n",
    "    if batch.id in desired_batch_ids:        \n",
    "        output_file = batch.output_file_id\n",
    "        content = client.files.content(output_file)    \n",
    "\n",
    "        jsonl_file_path = f'./data/step1/output/{output_file}.jsonl'\n",
    "        content.write_to_file(jsonl_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process AI responses and save data\n",
    "\n",
    "Now we have all the AI responses, we need to process and save the results. This\n",
    "will update the json file from step 2, and also export the responses as an Excel\n",
    "file for review. The Excel file will be located: `./data/step1/review1/xlsx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from config import AZURE_OPENAI_KEY, AZURE_OPENAI_BASE_URL\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "# Parses the JSON from a function call, if there is an error in JSON parsing, recalls the LLM with the fix json function to get a valid json response.\n",
    "def parse_JSON(json_str: str) -> dict:        \n",
    "    try: \n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:              \n",
    "        messages = [\n",
    "      {\n",
    "        'role': 'system',\n",
    "        'content':\n",
    "          'Assistant is a large language model designed to fix and return correct JSON objects.',\n",
    "      },\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': f'ORIGINAL ERROR CONTAINING JSON OBJECT:\\n\\n{json_str}\\n\\nERROR MESSAGE: {e}',\n",
    "      },\n",
    "    ]\n",
    "        \n",
    "        tool_choices = [{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'fix_object',\n",
    "        'description':\n",
    "          'You will be given an incorrectly formed JSON Object and a error message. You must fix the incorrect JSON Object and return the valid JSON object.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'fixedJSON': {\n",
    "              'type': 'string',\n",
    "              'description': 'The reformated and error free JSON object. Return the JSON object only!',\n",
    "            },\n",
    "          },\n",
    "          'required': ['fixedJSON'],\n",
    "        },\n",
    "      },\n",
    "    }]                \n",
    "        response = client.chat.completions.create(\n",
    "                    model='gpt-4o-2024-05-13',\n",
    "                    messages=messages,                    \n",
    "                    max_tokens=4096,\n",
    "                    temperature=0,\n",
    "                    tools=tool_choices,\n",
    "                    tool_choice={ 'type': 'function', 'function': { 'name': 'fix_object' } },        \n",
    "                )        \n",
    "                \n",
    "        second_test_json = response.choices[0].message.tool_calls[0].function.arguments \n",
    "                  \n",
    "        to_return = json.loads(second_test_json)\n",
    "        return json.loads(to_return['fixedJSON'])\n",
    "\n",
    "output_folder = './data/step1/output'\n",
    "\n",
    "jsonl_files = [f for f in os.listdir(output_folder) if os.path.isfile(os.path.join(output_folder, f)) and f.endswith('.jsonl')]\n",
    "\n",
    "# Load original JSON list\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "def get_correct_category(AI_category):\n",
    "    AI_category = AI_category.lower()    \n",
    "    if AI_category == 'digital platform':\n",
    "        return 'platform'\n",
    "    if AI_category == 'civil society':\n",
    "        return 'civil'\n",
    "    return AI_category\n",
    "\n",
    "# Load the JSONL files\n",
    "for file in jsonl_files:\n",
    "    with open(f\"{output_folder}/{file}\", \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)            \n",
    "            item_key = item['custom_id']            \n",
    "            json_res = parse_JSON(item['response']['body']['choices'][0]['message']['tool_calls'][0]['function']['arguments'])\n",
    "            # grab the matching item in our list\n",
    "            list_item = next((x for x in list_data if x['uniqueId'] == item_key), None)\n",
    "            if list_item:\n",
    "              json_res['responder_category'] = get_correct_category(json_res['responder_category'])\n",
    "              if list_item['group'] == None:\n",
    "                list_item['group'] = json_res['responder_category']\n",
    "                list_item['metadata']['groupDefinedBy'] = 'AI'              \n",
    "              list_item['AI_response_general'] = json_res\n",
    "              list_item['AI_response_general']['system_fingerprint'] = item['response']['body']['system_fingerprint']\n",
    "\n",
    "# Save the updated list back to the json file\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list_data, f)\n",
    "\n",
    "# Export the list to an Excel file for review\n",
    "# Convert JSON to DataFrame\n",
    "df = pd.json_normalize(list_data)\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df.to_excel('./data/step1/review1.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
