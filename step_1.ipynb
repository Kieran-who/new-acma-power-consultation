{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1:\n",
    "\n",
    "In this step we carry out the following:\n",
    "\n",
    "1. We load the spreadsheet from Sharepoint which includes the list of\n",
    "   submissions, the human defined categories for submitters of interest and any\n",
    "   further annotations such as whether we are removing submissions from\n",
    "   analysis. We format this data into a json file, saved in\n",
    "   `./data/step1/list.json`. This JSON file allows for easier manipulation and\n",
    "   handling.\n",
    "2. Using the JSON file from step 1, we create a number of jsonl files that are\n",
    "   in the correct format for processing by OpenAI's batch API. We create\n",
    "   multiple jsonl files as each has to be less than 100mb in size.\n",
    "3. We upload the jsonl files, and trigger the batch processing of them. This can\n",
    "   take upwards of 24 hours.\n",
    "4. Once processing is complete, we download the completed responses for each\n",
    "   request, and update the json file from step 1 to include the AI returned\n",
    "   data. We also export this data in a spreadsheet for review\n",
    "   (`./data/step1/review1.xlsx`)\n",
    "\n",
    "After this step, not only do we have preliminary data for all submissions\n",
    "(answers to questions below), but we can also check if the AI has catagorised\n",
    "any of the unlabeled submissions (that we may have missed) into the categories\n",
    "of interest. Once we are settled on the categories, we can move onto `Step 2`\n",
    "which involves asking the AI the specific questions for each category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading spreadsheet\n",
    "\n",
    "[The spreadsheet](https://studentutsedu.sharepoint.com/:x:/r/sites/CentreforMediaTransition76/_layouts/15/doc2.aspx?sourcedoc=%7B26015E46-DC17-46F8-85CB-8FF7601BB93E%7D&file=List%20of%20all%20submissions.xlsx&action=default&mobileredirect=true&DefaultItemOpen=1&ct=1715733887026&wdOrigin=OFFICECOM-WEB.START.REC&cid=c80abce9-d7d3-419b-8b78-2504ae4ce71a&wdPreviousSessionSrc=HarmonyWeb&wdPreviousSession=a8d0354e-a231-454b-a8a5-18124a5f1983)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the files that the text was manually extracted for (as opposed to\n",
    "being extracted by the Marker package)\n",
    "\n",
    "```python\n",
    "list_of_files_manually_text_extracted = ['e656', '14193', '18110', '19712', '26222', '33824', '34418', '34756', 'e597' ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Load the Excel spreadsheet into a pandas DataFrame\n",
    "df = pd.read_excel('./data/step1/list.xlsx')\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    filename = filename.replace('acma2023-', '')\n",
    "    parts = filename.split('-')\n",
    "    name_parts = parts[1:]\n",
    "    name = ' '.join(name_parts).split('.')[0]\n",
    "    if name.find('anonymous') != -1:\n",
    "        name = 'anonymous'\n",
    "    return name.lower()\n",
    "\n",
    "missing_files = []\n",
    "manual_files = []\n",
    "\n",
    "list_of_files_manually_text_extracted = ['e656', '14193', '18110', '19712', '26222', '33824', '34418', '34756', 'e597' ]\n",
    "\n",
    "def check_file_exists(doc_id, folder_path = './data/files'):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(doc_id) and doc_id not in list_of_files_manually_text_extracted:\n",
    "            return True\n",
    "    if doc_id not in list_of_files_manually_text_extracted:\n",
    "        missing_files.append(doc_id)\n",
    "        return False\n",
    "    manual_files.append(doc_id)\n",
    "    return False\n",
    "\n",
    "formatted_data = []\n",
    "# Convert empty cells in 'Group', 'Comments', and 'Removed (Y)' columns to None\n",
    "for row in data:\n",
    "    if pd.isnull(row['Group']):\n",
    "        row['Group'] = None\n",
    "    if pd.isnull(row['Comments']):\n",
    "        row['Comments'] = None\n",
    "    if pd.isnull(row['Removed (Y)']):\n",
    "        row['Removed (Y)'] = None\n",
    "\n",
    "    exist = check_file_exists(row['UniqueID'])\n",
    "    \n",
    "    formatted_row = {\n",
    "        'uniqueId': row['UniqueID'],\n",
    "        'group': row['Group'],\n",
    "        'submitter': extract_name_from_filename(row['doc']),        \n",
    "        \"metadata\": {\n",
    "            \"groupDefinedBy\": \"human\" if row['Group'] else \"AI\",\n",
    "            \"removed\": row['Removed (Y)'],\n",
    "            \"comments\": row['Comments'],\n",
    "            \"text_extraction_method\": \"Marker2\" if exist else 'manual'\n",
    "        }\n",
    "    }\n",
    "    formatted_data.append(formatted_row)\n",
    "    \n",
    "# Save the data as a JSON file if it doesn't exist\n",
    "json_file = './data/step1/list.json'\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')  # Adjust to your local timezone\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "output = {\n",
    "    \"data\": formatted_data,\n",
    "    \"metadata\": {\n",
    "        \"excel_to_json\": {\n",
    "            \"timestamp\": current_time,\n",
    "            \"submissions_missing_files\": missing_files,\n",
    "            \"submissions_using_manually_extracted_files\": manual_files\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(output, f)\n",
    "\n",
    "print(F'{len(missing_files)} submissions missing files: {missing_files}')\n",
    "print(F'{len(manual_files)} submissions using manually extracted files: {manual_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. We now have a JSON file of objects with key value pairs in the below form. We now process this to jsonl form for batch processing\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"group\": \"string | null\",\n",
    "  \"submitter\": \"string\",\n",
    "  \"uniqueId\": \"string\",\n",
    "  \"metadata\": {\n",
    "    \"groupDefinedBy\": \"human or AI\",\n",
    "    \"removed\": \"string | null\",\n",
    "    \"comments\": \"string | null\",\n",
    "    \"text_extraction_method\": \"Marker2 | manual\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We ignore in this step any submissions we flag as removed.\n",
    "\n",
    "jsonl files saved to `./data/step1/toProcess`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "promt_file = 'prompt_no_guidance.txt'\n",
    "\n",
    "# Define the prompt for each individual request\n",
    "def prompt_formatted() -> str:    \n",
    "    # Read the first file and set a string variable\n",
    "    with open(promt_file, 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    with open('prompt_issues.md', 'r') as file:\n",
    "        issues = file.read()\n",
    "\n",
    "    # with open('prompt_guidance_note.md', 'r') as file:\n",
    "    #     guidance_note = file.read()\n",
    "\n",
    "    with open('prompt_fact_sheet.md', 'r') as file:\n",
    "        fact_sheet = file.read()\n",
    "\n",
    "    prompt = prompt.replace('|issues|', issues)\n",
    "    # prompt = prompt.replace('|guidance_note|', guidance_note)\n",
    "    prompt = prompt.replace('|fact_sheet|', fact_sheet)    \n",
    "\n",
    "    return prompt\n",
    "\n",
    "def get_submission(submission_text: str, submission_author: str):\n",
    "    prompt = \"\"\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION START *****************************************\\n\\n\"\n",
    "\n",
    "    prompt += f\"Submission from: {submission_author}\\n\\n\"\n",
    "    \n",
    "    prompt += submission_text\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION END *****************************************\\n\\n\"\n",
    "    return prompt\n",
    "\n",
    "def get_function():\n",
    "    with open('function.json', 'r') as f:\n",
    "        function = json.load(f)\n",
    "    return function\n",
    "\n",
    "def get_file_path(doc_id, folder_path = './data/files'):    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.startswith(doc_id):\n",
    "            return os.path.join(folder_path, file_name)\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "md_file_location = './data/files'\n",
    "\n",
    "file_counter = 0\n",
    "jsonl_file = f\"./data/step1/toProcess/jsonl_{file_counter}.jsonl\"\n",
    "\n",
    "skipped_files = []\n",
    "empty_files = []\n",
    "\n",
    "# Make step 1 directories\n",
    "os.makedirs('./data/step1/toProcess', exist_ok=True)\n",
    "os.makedirs('./data/step1/output', exist_ok=True)\n",
    "\n",
    "prompt = prompt_formatted()\n",
    "\n",
    "# This loop takes each submission and adds it to the jsonl file in a format that can be used by the OpenAI API\n",
    "for i in list[\"data\"]:   \n",
    "    if i[\"metadata\"][\"removed\"] == \"Y\":\n",
    "        skipped_files.append(i[\"uniqueId\"])\n",
    "        continue\n",
    "    try:\n",
    "        md_file_path = get_file_path(i.get(\"uniqueId\"))       \n",
    "        with open(md_file_path, 'r') as file:\n",
    "            submission = file.read()        \n",
    "        if len(submission.strip()) == 0:\n",
    "            i[\"metadata\"][\"removed\"] = \"Y\"\n",
    "            i[\"metadata\"][\"comments\"] = f\"{i[\"metadata\"][\"comments\"]}\\n\\nRemoved due to empty file\"\n",
    "            empty_files.append(i[\"uniqueId\"])\n",
    "            continue\n",
    "        sub_author = i[\"submitter\"]\n",
    "        submission_formatted = get_submission(submission, sub_author)\n",
    "        function = get_function()\n",
    "\n",
    "        ldata = { \"custom_id\": i[\"uniqueId\"], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": submission_formatted}], \"max_tokens\": 4096, \"temperature\": 1e-9, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"top_p\": 0, \"tools\":[function], \"tool_choice\": { \"type\": \"function\", \"function\": { \"name\": \"submission_eval\" } }}}\n",
    "\n",
    "        i[\"metadata\"][\"SUBMISSION_CONTENT\"] =  submission_formatted\n",
    "        \n",
    "        if os.path.exists(jsonl_file) and os.path.getsize(jsonl_file) >= 85 * 1024 * 1024:  # \n",
    "            file_counter += 1\n",
    "            jsonl_file = f\"./data/step1/toProcess/jsonl_{file_counter}.jsonl\"\n",
    "        \n",
    "        i[\"metadata\"][\"step_1\"] = {\"batch_file\": f'jsonl_{file_counter}.jsonl'}\n",
    "        \n",
    "        with open(jsonl_file, 'a') as f:\n",
    "            json.dump(ldata, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "list[\"metadata\"][\"step_1\"] = { \"jsonl_batch_creation\": {\n",
    "            \"timestamp\": current_time,\n",
    "            \"skipped_files\": skipped_files,\n",
    "            \"empty_files\": empty_files,\n",
    "            \"total_jsonL_files\": file_counter + 1,\n",
    "            }, \n",
    "            \"ai_parameters\" : { \"custom_id\": \"SUBMISSION_ID\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"system\", \"content\": prompt},{\"role\": \"user\", \"content\": \"SUBMISSION_CONTENT\"}], \"max_tokens\": 4096, \"temperature\": 1e-9, \"frequency_penalty\": 0, \"presence_penalty\": 0, \"top_p\": 0, \"tools\":[function], \"tool_choice\": { \"type\": \"function\", \"function\": { \"name\": \"submission_eval\" } }}}\n",
    "        }\n",
    "\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list, f)\n",
    "\n",
    "print(f\"Empty files: {empty_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. We now have a folder with all the prepared files for OpenAI batch calls\n",
    "\n",
    "We will upload each of these files to OpenAI and then trigger batch processing\n",
    "of each.\n",
    "\n",
    "**MAKE SURE TO RECORD BATCH IDs CREATED IN THIS STEP SO WE KNOW WHICH FILES TO\n",
    "EVENTUALLY DOWNLOAD**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'), max_retries=3)\n",
    "\n",
    "jsonl_dir = './data/step1/toProcess'\n",
    "\n",
    "jsonl_files = [f for f in os.listdir(jsonl_dir) if os.path.isfile(os.path.join(jsonl_dir, f)) and f.endswith('.jsonl')]\n",
    "\n",
    "file_ids = []\n",
    "\n",
    "for file in jsonl_files:\n",
    "    file_object = client.files.create(\n",
    "        file=open(f\"{jsonl_dir}/{file}\", \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    file_ids.append(file_object.id)\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "list[\"metadata\"][\"step_1\"][\"file_upload\"] = {\n",
    "            \"timestamp\": current_time,\n",
    "            \"file_ids\": file_ids,            \n",
    "        }\n",
    "\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'), max_retries=3)\n",
    "\n",
    "# We have now uploaded all the files and have their IDs, lets create a batch job for each\n",
    "batch_ids = []\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "file_ids = list[\"metadata\"][\"step_1\"][\"file_upload\"][\"file_ids\"]\n",
    "\n",
    "for file_id in file_ids:\n",
    "    job = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "          )\n",
    "    batch_ids.append(job.id)\n",
    "\n",
    "print('Record the following and make sure to add to `desired_batch_ids` in the following cells!')\n",
    "print(batch_ids)\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "list[\"metadata\"][\"step_1\"][\"batch_creation\"] = {\n",
    "            \"timestamp\": current_time,\n",
    "            \"batch_ids\": batch_ids,\n",
    "        }\n",
    "\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The batch processes should now be underway, they will take up to 24hrs\n",
    "\n",
    "We can run the following cell to check on process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "desired_batch_ids = list[\"metadata\"][\"step_1\"][\"batch_creation\"][\"batch_ids\"]\n",
    "\n",
    "batch_jobs = client.batches.list()\n",
    "\n",
    "for batch in batch_jobs.data:\n",
    "    if batch.id in desired_batch_ids:\n",
    "        print(batch.id, batch.status, batch.request_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once processing is done, we can download the completed files\n",
    "\n",
    "Files are saved here: `./data/step1/output`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "batch_jobs = client.batches.list()\n",
    "\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "# we only want to download the batch jobs that were set up in cell 11\n",
    "desired_batch_ids = list[\"metadata\"][\"step_1\"][\"batch_creation\"][\"batch_ids\"]\n",
    "\n",
    "success_count = 0\n",
    "error_count = 0\n",
    "\n",
    "created_time = None\n",
    "completion_time = None\n",
    "\n",
    "for batch in batch_jobs.data:\n",
    "    if batch.id in desired_batch_ids:\n",
    "        # Gets the first created batch time\n",
    "        if not created_time or batch.created_at < created_time:\n",
    "            created_time = batch.created_at\n",
    "        # Gets the lasted completed batch time\n",
    "        if not completion_time or batch.completed_at > completion_time:\n",
    "            completion_time = batch.completed_at\n",
    "        success_count += batch.request_counts.total - batch.request_counts.failed\n",
    "        output_file = batch.output_file_id\n",
    "        content = client.files.content(output_file)\n",
    "        jsonl_file_path = f'./data/step1/output/{output_file}.jsonl'\n",
    "        content.write_to_file(jsonl_file_path)\n",
    "        # Handle error files\n",
    "        error_count += batch.request_counts.failed\n",
    "        err_file = batch.error_file_id\n",
    "        err_content = client.files.content(err_file)\n",
    "        err_jsonl_file_path = f'./data/step1/output/err_{err_file}.jsonl'\n",
    "        err_content.write_to_file(err_jsonl_file_path)\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "time_difference = completion_time - created_time\n",
    "\n",
    "list[\"metadata\"][\"step_1\"][\"batch_creation\"][\"download_timestamp\"] = current_time\n",
    "list[\"metadata\"][\"step_1\"][\"batch_creation\"][\"completetion_duration_seconds\"] = time_difference\n",
    "list[\"metadata\"][\"step_1\"][\"batch_creation\"][\"success\"] = success_count\n",
    "list[\"metadata\"][\"step_1\"][\"batch_creation\"][\"errors\"] = error_count\n",
    "\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process AI responses and save data\n",
    "\n",
    "Now we have all the AI responses, we need to process and save the results. This\n",
    "will update the json file from step 2, and also export the responses as an Excel\n",
    "file for review. The Excel file will be located: `./data/step1/review1/xlsx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "# Parses the JSON from a function call, if there is an error in JSON parsing, recalls the LLM with the fix json function to get a valid json response.\n",
    "def parse_JSON(json_str: str) -> dict:        \n",
    "    try: \n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:              \n",
    "        messages = [\n",
    "      {\n",
    "        'role': 'system',\n",
    "        'content':\n",
    "          'Assistant is a large language model designed to fix and return correct JSON objects.',\n",
    "      },\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': f'ORIGINAL ERROR CONTAINING JSON OBJECT:\\n\\n{json_str}\\n\\nERROR MESSAGE: {e}',\n",
    "      },\n",
    "    ]\n",
    "        \n",
    "        tool_choices = [{\n",
    "      'type': 'function',\n",
    "      'function': {\n",
    "        'name': 'fix_object',\n",
    "        'description':\n",
    "          'You will be given an incorrectly formed JSON Object and a error message. You must fix the incorrect JSON Object and return the valid JSON object.',\n",
    "        'parameters': {\n",
    "          'type': 'object',\n",
    "          'properties': {\n",
    "            'fixedJSON': {\n",
    "              'type': 'string',\n",
    "              'description': 'The reformated and error free JSON object. Return the JSON object only!',\n",
    "            },\n",
    "          },\n",
    "          'required': ['fixedJSON'],\n",
    "        },\n",
    "      },\n",
    "    }]                \n",
    "        response = client.chat.completions.create(\n",
    "                    model='gpt-4o-2024-05-13',\n",
    "                    messages=messages,                    \n",
    "                    max_tokens=4096,\n",
    "                    temperature=0,\n",
    "                    tools=tool_choices,\n",
    "                    tool_choice={ 'type': 'function', 'function': { 'name': 'fix_object' } },        \n",
    "                )        \n",
    "                \n",
    "        second_test_json = response.choices[0].message.tool_calls[0].function.arguments \n",
    "                  \n",
    "        to_return = json.loads(second_test_json)\n",
    "        return json.loads(to_return['fixedJSON'])\n",
    "\n",
    "output_folder = './data/step1/output'\n",
    "\n",
    "jsonl_files = [f for f in os.listdir(output_folder) if os.path.isfile(os.path.join(output_folder, f)) and f.endswith('.jsonl')]\n",
    "\n",
    "# Load original JSON list\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "def get_correct_category(AI_category):\n",
    "    AI_category = AI_category.lower()    \n",
    "    if AI_category == 'digital platform':\n",
    "        return 'platform'\n",
    "    if AI_category == 'civil society':\n",
    "        return 'civil'\n",
    "    return AI_category\n",
    "\n",
    "prompt_tokens = 0\n",
    "completion_tokens = 0\n",
    "total_tokens = 0\n",
    "\n",
    "# Load the JSONL files\n",
    "for file in jsonl_files:    \n",
    "    with open(f\"{output_folder}/{file}\", \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)            \n",
    "            item_key = item['custom_id']            \n",
    "            # grab the matching item in our list            \n",
    "            list_item = next((x for x in list_data[\"data\"] if x['uniqueId'] == item_key), None)\n",
    "            if list_item:\n",
    "              if item[\"response\"][\"status_code\"] != 200:\n",
    "                list_item[\"step_1\"] = None\n",
    "                list_item[\"metadata\"][\"step_1\"][\"error\"] = item[\"response\"]\n",
    "                continue                \n",
    "              json_res = parse_JSON(item['response']['body']['choices'][0]['message']['tool_calls'][0]['function']['arguments'])\n",
    "              if 'responder_category' in json_res:\n",
    "                json_res['responder_category'] = get_correct_category(json_res['responder_category'])\n",
    "                if list_item['group'] == None:\n",
    "                  list_item['group'] = json_res['responder_category']\n",
    "                  list_item['metadata']['aiResponderCategory'] = 'AI'\n",
    "              list_item[\"step_1\"] = json_res\n",
    "              list_item[\"metadata\"][\"step_1\"][\"system_fingerprint\"] = item['response']['body']['system_fingerprint']\n",
    "              list_item[\"metadata\"][\"step_1\"][\"batch_id\"] = item['id']\n",
    "              prompt_tokens += item['response']['body'][\"usage\"][\"prompt_tokens\"]\n",
    "              completion_tokens += item['response']['body'][\"usage\"][\"completion_tokens\"]\n",
    "              total_tokens += item['response']['body'][\"usage\"][\"total_tokens\"]\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "list_data[\"metadata\"][\"step_1\"][\"batch_processed\"] = {\"timestamp\": current_time}\n",
    "list_data[\"metadata\"][\"step_1\"][\"usage\"] = {\"prompt_tokens\": prompt_tokens, \"completion_tokens\": completion_tokens, \"total_tokens\": total_tokens}\n",
    "\n",
    "# Save the updated list back to the json file\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list_data, f)\n",
    "\n",
    "# Export the list to an Excel file for review\n",
    "# Convert JSON to DataFrame\n",
    "df = pd.json_normalize(list_data[\"data\"])\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df.to_excel('./data/step1/step1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review AI Classification\n",
    "\n",
    "This is the last stage of step 1. Here we list and review cases where the AI has\n",
    "classified a submission into one of our categories of interest (news, academic,\n",
    "civil, platform or industry) and where we had not originally classified them as\n",
    "so.\n",
    "\n",
    "The next two cells do two things:\n",
    "\n",
    "- **Cell one:** Will list the submission numbers for each category where the AI\n",
    "  has provided a categorisation of interest\n",
    "- **Cell two:** For those listed submissions, we can manually update the\n",
    "  classification as required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 (one)\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Load original JSON list\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "# Filter AI-generated items which are grouped and not 'individual'\n",
    "ai_generated_items = [item for item in list_data[\"data\"] if item['metadata']['groupDefinedBy'] == 'AI' and item['group'] and item['group'] != 'individual']\n",
    "\n",
    "# Create a dictionary to store the grouped items\n",
    "grouped_items_dict = {}\n",
    "\n",
    "# Group the items based on the 'group' value\n",
    "for item in ai_generated_items:\n",
    "    group = item['group']\n",
    "    if group not in grouped_items_dict:\n",
    "        grouped_items_dict[group] = []\n",
    "    grouped_items_dict[group].append(item['uniqueId'])\n",
    "\n",
    "# Print the grouped items\n",
    "for group, items in grouped_items_dict.items():\n",
    "    print(f\"Group: {group}\")\n",
    "    for item in items:\n",
    "        print(item)\n",
    "\n",
    "# Get the current time in the specified timezone\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "# Update the metadata with the classification check\n",
    "list_data[\"metadata\"][\"step_1\"][\"classification_check\"] = {\n",
    "    \"timestamp\": current_time,\n",
    "    \"groups_to_review\": grouped_items_dict\n",
    "}\n",
    "\n",
    "# Save the updated list back to the json file\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 (two) - manual update with correct classification\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# The below lists are the items that need to be manually updated with the correct classification\n",
    "# Add to each list the uniqueId of the item that needs to be updated to the category matching the list (i.e. if submission '30105' should be in the 'academic' category, add '30105' to the 'academic' list)\n",
    "\n",
    "academic = []\n",
    "civil = []\n",
    "platform = []\n",
    "news = []\n",
    "individual = []\n",
    "industry = []\n",
    "\n",
    "# Load original JSON list\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list_data = json.load(f)\n",
    "\n",
    "def update_classification(category_list, group_name):\n",
    "    for item in category_list:\n",
    "        list_item = next((x for x in list_data if x['uniqueId'] == item), None)\n",
    "        if list_item:\n",
    "            list_item['group'] = group_name\n",
    "            list_item['metadata']['groupDefinedBy'] = 'human'\n",
    "\n",
    "update_classification(academic, 'academic')\n",
    "update_classification(civil, 'civil')\n",
    "update_classification(platform, 'platform')\n",
    "update_classification(news, 'news')\n",
    "update_classification(individual, 'individual')\n",
    "update_classification(industry, 'industry')\n",
    "\n",
    "local_timezone = pytz.timezone('Australia/Sydney')\n",
    "current_time = datetime.now(local_timezone).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "list_data[\"metadata\"][\"step_1\"][\"classification_check\"][\"human_review\"] = {\n",
    "    \"timestamp\":current_time,\n",
    "    \"fixed\": {\n",
    "        \"academic\": academic,\n",
    "        \"civil\": civil,\n",
    "        \"platform\": platform,\n",
    "        \"news\": news,\n",
    "        \"individual\": individual,\n",
    "        \"industry\": industry\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Save the updated list back to the json file\n",
    "with open('./data/step1/list.json', 'w') as f:\n",
    "    json.dump(list_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
