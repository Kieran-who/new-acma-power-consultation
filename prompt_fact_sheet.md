# Communications Legislation Amendment (Combatting Misinformation and Disinformation) Bill 2023—Fact sheet

**June 2023**

## Key points

- Misinformation and disinformation pose a threat to the safety and wellbeing of
  Australians, as well as our democracy, society and economy.
- The Australian Government has released the draft Communications Legislation
  Amendment (Combatting Misinformation and Disinformation) Bill 2023 (the Bill)
  to address this growing challenge.
- The Bill would give the Australian Communications and Media Authority (ACMA)
  reserve powers to act, if industry efforts in regard to misinformation and
  disinformation are inadequate.
- The proposed powers would:
- enable the ACMA to gather information from, or require digital platform
  providers to keep certain records about matters regarding misinformation and
  disinformation
- enable the ACMA to request industry develop a code of practice covering
  measures to combat misinformation and disinformation on digital platforms,
  which the ACMA could register and enforce
- allow the ACMA to create and enforce an industry standard (a stronger form of
  regulation), should a code of practice be deemed ineffective in combatting
  misinformation and disinformation on digital platforms.
- The ACMA will not have the power to request specific content or posts be
  removed from digital platform services.
- The Bill defines misinformation and disinformation as follows:
- Misinformation is online content that is false, misleading or deceptive, that
  is shared or created without an intent to deceive but can cause and contribute
  to serious harm.
- Disinformation is misinformation that is intentionally disseminated with the
  intent to deceive or cause serious harm.
- Serious harm is harm that affects a significant portion of the Australian
  population, economy or environment, or undermines the integrity of an
  Australian democratic process.
- The powers apply to digital platform services that are accessible in
  Australia. Some examples include social media, search engines, instant
  messaging services (although the content of private messages will be out of
  scope), news aggregators and podcasting services.
- The Bill includes strong protections for privacy and freedom of speech:
- the Bill is directed at encouraging digital platform providers to have robust
  systems and measures in place to address misinformation and disinformation on
  their services, rather than the ACMA directly regulating individual pieces of
  content
- the ACMA will not have the power to request specific content or posts be
  removed from digital platform services
- rules made under the Bill may require digital platform services to have
  systems and processes in place to address misinformation or disinformation
  that meets a threshold of being likely to cause or contribute to serious harm
- the code and standard-making powers will not apply to authorised electoral and
  referendum content and other types of content such as professional news and
  satire
- private messages sent on instant messaging services will not be within scope
  of the powers.
- You are invited to comment on the draft Bill before it is finalised and
  introduced in Parliament later this year to ensure that it strikes an
  appropriate balance in protecting Australians from harm.

## Why these powers are needed

Misinformation and disinformation spread via digital platform services is a
major issue worldwide. The rapid spread of false, misleading and deceptive
information online has resulted in a multitude of harms from disrupted public
health responses to foreign interference in elections and the undermining of
democratic institutions.

In 2021, the ACMA produced its _Report to government on the adequacy of digital
platforms’ disinformation and news quality measures_:

- The report noted existing efforts by signatories to the voluntary industry
  code (the Australian Code of Practice on Disinformation and Misinformation)
  were a good first step in efforts to tackle misinformation and disinformation
  on digital platform services.
- However, the ACMA recommended the government provide it with a graduated set
  of new powers to combat misinformation and disinformation across the sector.
  These powers would increase transparency and ensure that digital platform
  services are held to account if voluntary industry efforts prove to be
  inadequate.

As a key principle, the proposed ACMA powers will support the success of the
voluntary industry code currently in place. Digital Industry Group Inc. (DIGI),
the industry body responsible for the current code, has expressed in principle
support for the proposed powers outlined in the ACMA’s report.

## What’s in scope

### Misinformation and disinformation

Misinformation is:

- content disseminated using a digital service that is false, misleading or
  deceptive; and
- the content is provided on the digital service to one or more end-users in
  Australia; and
- the provision of the content on the digital service is reasonably likely to
  cause or contribute to serious harm; and
- the content is not excluded for misinformation purposes, with that content
  being:

  - content produced in good faith for the purposes of entertainment, parody or
    satire
  - professional news content
  - content authorised by the Australian or a, State, Territory or Local
    Government
  - content produced by or for an accredited education provider.

NOTE: Content made in response to the above excluded types of content is not
automatically also excluded for misinformation purposes (i.e. comments on a
professional news article).

Disinformation is:

- content that fulfils the criteria for misinformation; and
- the content is disseminated with intent to deceive, including through
  automated processes and foreign interference. This captures content that is
  purposefully or maliciously disseminated disinformation.

### Serious harm

The proposed powers will only apply to misinformation and disinformation that is
reasonably likely to cause or contribute to serious harm. The matters that are
relevant to determining whether the content is reasonably likely to cause or
contribute to serious harm are outlined in section 2.1.2 of the Guidance Note to
the Bill, and clause 7 of the Bill.

The types of ‘harms’ captured, and some examples of what could be serious harm,
are set out below:

| **Type of harm**                                                                                                                                                      | **Example of serious harm**                                                                                                 |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| Hatred against a group in Australian society on the basis of ethnicity, nationality, race, gender, sexual orientation, age, religion or physical or mental disability | Misinformation about a group of Australians inciting other persons to commit hate crimes against that group                 |
| Disruption of public order or society in Australia                                                                                                                    | Misinformation that encouraged or caused people to vandalise critical communications infrastructure                         |
| Harm to the integrity of Australian democratic processes or of Commonwealth, State, Territory or local government institutions                                        | Misinformation undermining the impartiality of an Australian electoral management body ahead of an election or a referendum |
| Harm to the health of Australians                                                                                                                                     | Misinformation that caused people to ingest or inject bleach products to treat a viral infection                            |
| Harm to the Australian environment                                                                                                                                    | Misinformation about water saving measures during a prolonged drought period in a major town or city                        |
| Economic or financial harm to Australians, the Australian economy or a sector of the Australian economy                                                               | Disinformation by a foreign actor targeting local producers in favour of imported goods                                     |

#### Would the powers address the targeting of an individual such as racist trolling?

The proposed powers are designed to encourage digital platform services to be
accountable for improving and implementing measures to counter the spread or
misinformation and disinformation online (i.e. they have a ‘systems’ focus
rather than an individual content focus).

Widespread, race-based misinformation and disinformation that caused or
contributed to serious harm would fall within the scope of the new ACMA powers.

For cases where a specific individual is subject to race-based harassment, abuse
or trolling, this would be a matter for the eSafety Commissioner to consider in
the context of the Commissioner’s powers to address adult cyber abuse under the
[_Online Safety Act 2021_](https://www.legislation.gov.au/Details/C2021A00076).
In such instances, when platforms have failed to remove abusive content, an
individual can report it to the eSafety Commissioner at
[esafety.gov.au](https://www.esafety.gov.au). The eSafety Commissioner has
powers to require digital platform services to remove adult cyber abuse content.

### Digital platform services in scope

The powers would apply to a broad range of digital platform services. This
includes search engines, news aggregators, instant messaging services, social
media, web-forums, dating sites, and online peer-to-peer marketplaces.

The powers would not apply to SMS and MMS (text messages sent via mobile
telecommunications networks), email, SVODs (subscription video on demand e.g.
Netflix), and BVODs (broadcast video on demand e.g. ABC iView).

Further information about how services are defined is in section 2.1.1 of the
Guidance Note to the Bill and clauses 3, 4, 5 and 6 of the Bill.

#### Private messages

##### Private messages on a digital platform service are out of scope

The ACMA powers would not apply to direct private messages sent from one user to
one or more other users on a messaging service or social media platform or the
content of a closed group conversation on a messaging service such as a family
group chat.

##### Group chats open to the public on an instant messaging service will be within scope

The content of group chats that are open to the public or public “channels” on
instant messaging services are intended to be within scope of the ACMA powers.
This is also the case for posts in a forum or message board. In these cases,
digital platform services will be responsible for ensuring they prevent and
respond to misinformation and disinformation on their services.

##### If private messages are exempt, how will the powers apply to instant messaging services?

While the content of private messages will be exempt from the scope of the
powers, the ACMA would be able to use its information-gathering and recording
keeping powers. This is to understand the measures that digital platforms take
on their services to combat the spread of misinformation and disinformation and
to gain a better understanding of the number of complaints made about such
content on their services.

These powers will not require providers of digital platform services to reveal
the contents of private messages or have requirements related to breaking
encryption of private messages.

To strengthen their ability to combat misinformation and disinformation,
providers of digital platform services may choose to have systems and processes
in place such as user reporting tools, complaints handling and educative
programs to empower users. These requirements may also be articulated in
industry codes and standards made under the Bill.

Further information on private messages refer to section 2.1.3 in the Guidance
Note to the Bill, and clauses 2 and 34, and subclauses 14(3), 18(4), 19(4) of
the Bill.

## How the ACMA powers will be structured

### Information-gathering and record keeping powers

#### Record keeping rules

The ACMA would have a power to make rules requiring providers of particular
digital platform services (or classes of services) to maintain and keep records
relating to:

- misinformation or disinformation on their services
- measures implemented by digital platform services to prevent or respond to
  misinformation or disinformation on their services, and the effectiveness of
  those measures
- the prevalence of content containing false, misleading or deceptive
  information provided on their digital platform services.

Digital platforms providers may be required to periodically report on this to
the ACMA. This would enhance transparency and allow tracking of digital
platforms’ progress in addressing misinformation and disinformation on their
services.

#### Information-gathering powers

The ACMA would have powers to obtain information, documents and evidence from
providers of digital platform services when needed for investigating matters
relating to the same type mentioned above.

The ACMA may also obtain information from other persons to assist the ACMA
monitor compliance with misinformation codes, misinformation standards and
digital platform rules. They could include fact-checkers or other third-party
contractors to digital platform service providers. The ACMA may only do this if
it considers it requires it for its monitoring and compliance functions.

#### Publication of information

To promote transparency, the ACMA would have the ability to publish information
collected under its information-gathering and record keeping powers on its
website, including the identity of the provider or service to which the
information relates.

The ACMA would not be permitted to publish personal information and will be
required to consult with impacted digital platform service providers prior to
publishing any information.

Further information on the above powers is in section 3 of the Guidance Note to
the Bill, and clause 25 of the Bill.

### Code and standard-making powers

Under the framework of the Bill, the ACMA would have reserve powers to register
codes and make standards to compel digital platform service providers to act
against misinformation and disinformation on their services.

These powers would be used in the event that the ACMA determines that existing
industry efforts to combat misinformation and disinformation on digital platform
services do not provide adequate community protections.

The ACMA would have a range of enforcement powers to ensure that digital
platform providers comply. Further information on these powers is detailed below
and in section 4 of the Guidance Note to the Bill, and part 3 of the Bill.

#### Examples of matters that may be dealt with in a registered code or standard

- preventing or responding to misinformation or disinformation on digital
  platforms services
- using technology to prevent or respond to misinformation or disinformation on
  digital platform services
- preventing or responding to misinformation or disinformation on digital
  platform services that constitutes an act of foreign interference
- preventing advertising involving misinformation or disinformation on digital
  platform services
- preventing monetisation of misinformation or disinformation on digital
  platform services
- allowing end-users to detect and report misinformation or disinformation on
  digital platform services
- policies and procedures for receiving and handling reports and complaints from
  end-users

#### Code of practice registration

Should the ACMA determine that stronger action is needed to protect Australians,
it could request that a section of the industry put in place a new and more
effective code of practice (than the existing DIGI voluntary code of practice,
for example). Once the ACMA is satisfied a draft code presented to it by
industry meets a number of criteria, it may register it which makes compliance
with it compulsory for all digital services providers in the relevant segment of
the industry. This would include those providers who chose not to sign up to a
voluntary code.

#### Standards

In the event previous efforts through a code had not been effective, or a code
was not developed, or otherwise in urgent and exceptional circumstances, the
ACMA would have the power to make an enforceable standard.

A standard would be a determination written by the ACMA that would require
digital platform providers to combat misinformation and disinformation on their
services. Such a standard would have higher penalties than registered codes and
would generally reflect a determination that previous efforts had not been
effective.

#### Consultation

Public consultation by the industry body or association producing a code is a
requirement prior to the ACMA registering it. If the ACMA were to make a
standard, appropriate consultation, would also occur under the provisions of the
Bill.

### Enforcement

In the event of non-compliance with the information-gathering and record keeping
rules, codes or standards, the ACMA would be able to choose from a range of
formal enforcement actions.

These actions would generally be applied in a graduated manner, dependent on the
harm caused, or risk of harm and could include issuing formal warnings,
infringement notices, remedial directions, injunctions and civil penalties.

Criminal penalties would only apply to digital platforms or individuals
knowingly making or retaining false or misleading records under the record
keeping provisions, or giving false or misleading information or evidence under
the information-gathering provisions of the new powers.

Digital platform providers can face significant civil penalties under the Bill,
and it is expected that the ACMA will actively seek penalty orders against those
providers who routinely contravene provisions in a registered code or a
standard, or fail to comply with remedial directions in particular.

The maximum amount of civil penalties is intended to deter systemic
non-compliance by digital platform providers and reflects the serious large
scale social, economic and/or environmental harms and consequences that could
result from the spread of misinformation or disinformation.

The civil penalties for breaches of standards are greater than breaches of codes
(or information-gathering powers) as a standard is the highest level of
regulatory action in the regulatory framework.

| **Maximum penalties – non-compliance with registered code**                                                                                                                                     | **Maximum penalties – non-compliance with industry standard**                                                                                                                                   |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Maximum of 10,000 penalty units ($2.75 million in 2023) or 2 per cent of global turnover (whatever is greater) for corporations or 2,000 penalty units ($0.55 million in 2023) for individuals. | Maximum of 25,000 penalty units ($6.88 million in 2023) or 5 per cent of global turnover (whatever is greater) for corporations or 5,000 penalty units ($1.38 million in 2023) for individuals. |

Further details on the enforcement mechanisms are in section 5 of the Guidance
Note to the Bill.

## Protecting privacy and freedom of expression

In seeking to implement regulatory measures to ensure digital platform providers
actively combat misinformation and disinformation on their services, the
government is committed to achieving a balance that upholds the rights and
freedoms of Australians whilst protecting Australians from serious harm that can
come from the spread of misinformation and disinformation.

### Private messaging

The Bill excludes the ACMA from being able to use its information-gathering
powers to require a person to give information or evidence, or produce a
document that would reveal the contents of private messages between users.

Similarly, the ACMA cannot create record keeping rules that require digital
platform providers to make or retain records of the contents of private
messages. Misinformation codes and standards will not be able to require
platforms to break encryption or to monitor private messages.

### Freedom of expression

The ACMA would have no role in determining truthfulness, nor will it have a role
in taking down or requesting action regarding individual pieces of content.

If the ACMA uses its reserve code registration or standard-making powers, it
will be required to consider whether there are any potential burdens on freedom
of political communication, and if so, to consider whether they are reasonable
and not excessive, in view of intended the protection from serious harms.

As proposed in the Bill, the proposed ACMA powers will:

- focus on ensuring digital platform providers have **systems** and measures in
  place to combat **misinformation and disinformation** on their services which
  pose a risk of serious harm.
- require **digital platform services to continue to be responsible** for the
  content they host and promote to users.
- not apply to professional news content and the other types of excluded content
  (noted above).

## The code and standard-making powers will not apply to electoral and referendum communications that are required to be authorised

## Complaints mechanisms

In the first instance, any online user complaints about the content on a digital
platform service or the policies and the terms of service of a platform, should
be directed to the digital platform provider.

The ACMA may investigate potential breaches of codes or standards made under the
Bill. Complaints about systemic or a regular pattern of misinformation or
disinformation on a service may be a trigger for the ACMA to investigate a
digital platform provider’s compliance with a code or standard.

- The record keeping and information-gathering powers could also enable the ACMA
  to determine whether digital platform providers’ complaints handling
  procedures are effective in combatting misinformation and disinformation on
  their services.
- The code and standard-making powers could be used to require the platforms to
  have policies and procedures for receiving and handling reports and complaints
  from end-users.

## Interaction with the voluntary code

### Will voluntary codes in place before the ACMA powers come into effect be automatically replaced by the new powers?

No. The Bill seeks to incentivise and strengthen the voluntary framework. The
ACMA would work with industry to ensure continuous improvement to the voluntary
code which is overseen by industry.

However, should those efforts prove inadequate, the ACMA would have the option
to use the graduated set of reserve powers to ask industry to make a new,
registrable code, or if necessary, the ACMA could make a standard.

### Will a voluntary code need to adopt the definitions in the Bill?

No. As the ACMA has no role in determining the provisions within any voluntary
codes, the industry does not need to adopt definitions in the Bill.

If the ACMA were to register a code, then it would need to draw upon the Bill’s
definitions.
