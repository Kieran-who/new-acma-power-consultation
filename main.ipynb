{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the attitudes towards the proposed new ACMA powers to combat misinformation and disinformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate prompt\n",
    "\n",
    "This function takes the submission and submission author (from doc name) as a\n",
    "parameter and returns the formatted prompt to be sent to the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatted(submission_string: str, submission_author: str) -> str:    \n",
    "    # Read the first file and set a string variable\n",
    "    with open('prompt.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    with open('prompt_issues.md', 'r') as file:\n",
    "        issues = file.read()\n",
    "\n",
    "    with open('prompt_guidance_note.md', 'r') as file:\n",
    "        guidance_note = file.read()\n",
    "\n",
    "    with open('prompt_fact_sheet.md', 'r') as file:\n",
    "        fact_sheet = file.read()\n",
    "\n",
    "    prompt = prompt.replace('|issues|', issues)\n",
    "    prompt = prompt.replace('|guidance_note|', guidance_note)\n",
    "    prompt = prompt.replace('|fact_sheet|', fact_sheet)\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION START *****************************************\\n\\n\"\n",
    "\n",
    "    prompt += f\"Submission from: {submission_author}\\n\\n\"\n",
    "    \n",
    "    prompt += submission_string\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION END *****************************************\\n\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AI response\n",
    "\n",
    "This function calls the AI model to elicit a response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/k/.cache/weaviate-embedded: process ID 29309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-05-08T10:58:33+10:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-05-08T10:58:33+10:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-05-08T10:58:33+10:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-05-08T10:58:33+10:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50050\",\"time\":\"2024-05-08T10:58:33+10:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-05-08T10:58:33+10:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error setting up classes\n",
      "string indices must be integers, not 'str'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8813dffe234cf8b83d8c84c738fef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/2015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 80.48%, threshold set to 80.00%\",\"path\":\"./db/data\",\"time\":\"2024-05-08T10:58:34+10:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard submission_ZLYAOU19n7NA in 28.722084ms\",\"time\":\"2024-05-08T10:58:34+10:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":13250,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-05-08T10:58:34+10:00\",\"took\":16036000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, your messages resulted in 332816 tokens (332049 in the messages, 767 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Error processing file: 34751-anonymous.md\n",
      "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, your messages resulted in 332816 tokens (332049 in the messages, 767 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "client initialized\n",
      "{'substantive_submission': True, 'responder_category': 'Individual', 'support': 'oppose', 'motivations': ['Freedom of speech', 'Democratic values', 'Government overreach'], 'regulation': \"The submission criticizes the bill for creating a discriminatory system that divides citizens into two classes, with politicians, journalists, and members of educational institutions in one class, and regular citizens in another. It argues that this system undermines freedom of speech and democratic ideals, and that the excessive fines will lead to stricter restrictions on speech. The submission also mentions the lack of 'pressure escape valves' within the code, which could compound the harm across the industry.\", 'perceived_impact': 'The submitter perceives the impact of the proposed laws as negative, arguing that it will disproportionately harm the voices of ordinary Australians and undermine the democratic ideals that have enabled open and honest discussions. The submission also raises concerns about the impossibility of accurately determining what is true or untrue, and the potential for content to be removed under the industry or mandatory codes created by ACMA. Furthermore, it highlights concerns about the delegation of legislative power to private entities and the implications for the implied constitutional freedom of political communication.', 'regulator_trust': \"The submission expresses skepticism towards the ACMA's ability to impartially and effectively use the new powers, citing concerns about the delegation of legislative power to private entities and the potential for limitations on discourse that unreasonably curtail the implied constitutional freedom of political communication. It also raises concerns about the bill's assumptions regarding the Government and its accredited media and educational institutions having the final say on what is true.\", 'author': 'amanda gorst', 'file_name': 'e2729-amanda-gorst'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"read_disk_use\",\"level\":\"warning\",\"msg\":\"disk usage currently at 80.39%, threshold set to 80.00%\",\"path\":\"./db/data\",\"time\":\"2024-05-08T10:59:04+10:00\"}\n"
     ]
    }
   ],
   "source": [
    "from az_client import call_ai, get_vector\n",
    "from tqdm.notebook import tqdm\n",
    "from db.docs import DocumentManager\n",
    "from db.db_instance import DBClient\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "client = DBClient()\n",
    "db = DocumentManager()\n",
    "\n",
    "with open('function.json', 'r') as f:\n",
    "    function = json.load(f)\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    parts = filename.split('-')\n",
    "    name_parts = parts[1:]\n",
    "    name = ' '.join(name_parts).split('.')[0]\n",
    "    return name\n",
    "\n",
    "def add_to_json(data, filename):\n",
    "    if not os.path.isfile('./data/processed_data.json'):\n",
    "        with open('./data/processed_data.json', 'w') as f:\n",
    "            json.dump([], f)    \n",
    "    with open('./data/processed_data.json', 'r') as f:\n",
    "        previous_data = json.load(f)\n",
    "        previous_data.append({filename: data})    \n",
    "    with open('./data/processed_data.json', 'w') as f:\n",
    "        json.dump(previous_data, f)\n",
    "\n",
    "def process_files_in_directory(directory, completed_directory):\n",
    "    # Check if the 'completed' directory exists, if not, create it\n",
    "    if not os.path.exists(completed_directory):\n",
    "        os.makedirs(completed_directory)\n",
    "    # Get a list of markdown files to process\n",
    "    markdown_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.endswith('.md')]\n",
    "    # Initialize the progress bar\n",
    "    for filename in tqdm(markdown_files, desc='Processing files'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "            with open(filepath, 'r') as file:\n",
    "                submission = file.read()\n",
    "            sub_author = extract_name_from_filename(filename)\n",
    "            prompt = prompt_formatted(submission, sub_author)\n",
    "            response = call_ai(prompt, function)\n",
    "            \n",
    "            response[\"author\"] = sub_author\n",
    "            response[\"file_name\"] = filename.replace('.md', '')\n",
    "\n",
    "            vector = get_vector(submission)\n",
    "\n",
    "            db.new_doc(response, vector, True)\n",
    "            add_to_json(response, filename.split('-')[0])\n",
    "            # Move the processed file to the 'completed' directory\n",
    "            completed_filepath = os.path.join(completed_directory, filename)\n",
    "            shutil.move(filepath, completed_filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {filename}\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "# Example usage\n",
    "directory = './data/files/converted'\n",
    "completed_directory = './data/files/completed'\n",
    "process_files_in_directory(directory, completed_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
