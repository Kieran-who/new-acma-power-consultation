{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicates\n",
    "\n",
    "The record for how duplicates are handled in a spreadsheet containing all\n",
    "submissions. This is downloaded and saved to JSON format in next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates via comparing doc hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "def hash_file(filepath):\n",
    "    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        # Read and update hash in chunks of 4K\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def find_duplicate_pdfs(folder_path):\n",
    "    \"\"\"Find duplicate PDFs in a folder.\"\"\"\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            file_hash = hash_file(filepath)\n",
    "            \n",
    "            if file_hash in hashes:\n",
    "                duplicates.append((filename, hashes[file_hash]))\n",
    "            else:\n",
    "                hashes[file_hash] = filename\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "folder_path = './static/submissions'\n",
    "duplicates = find_duplicate_pdfs(folder_path)\n",
    "\n",
    "if duplicates:\n",
    "    print(\"Found duplicates:\")\n",
    "    for dup in duplicates:\n",
    "        print(f\"{dup[0]} is a duplicate of {dup[1]}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List documents from same author for checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    parts = filename.split('-')\n",
    "    name_parts = parts[1:]\n",
    "    name = ' '.join(name_parts).split('.')[0]\n",
    "    if name.find('anonymous') != -1:\n",
    "        name = 'anonymous'\n",
    "    return name.lower()\n",
    "\n",
    "def find_duplicate_names(folder_path):\n",
    "    name_to_files = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            name = extract_name_from_filename(filename)\n",
    "            if name != 'anonymous':\n",
    "                if name in name_to_files:\n",
    "                    name_to_files[name].append(filename)\n",
    "                else:\n",
    "                    name_to_files[name] = [filename]\n",
    "    \n",
    "    duplicates = {name: files for name, files in name_to_files.items() if len(files) > 1}\n",
    "    return duplicates\n",
    "\n",
    "def copy_files_to_subfolders(folder_path, duplicates):\n",
    "    # Create a main directory for the duplicates\n",
    "    duplicates_dir = os.path.join(folder_path, 'duplicates')\n",
    "    if not os.path.exists(duplicates_dir):\n",
    "        os.makedirs(duplicates_dir)\n",
    "    \n",
    "    for name, files in duplicates.items():\n",
    "        # Create a subfolder for each duplicate name\n",
    "        subfolder_path = os.path.join(duplicates_dir, name)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "        \n",
    "        # Copy the files into their respective subfolders\n",
    "        for file in files:\n",
    "            src_path = os.path.join(folder_path, file)\n",
    "            dst_path = os.path.join(subfolder_path, file)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "folder_path = './static/submissions'\n",
    "duplicates = find_duplicate_names(folder_path)\n",
    "\n",
    "# Copy the files to subfolders\n",
    "copy_files_to_subfolders(folder_path, duplicates)\n",
    "\n",
    "print(f'Copied files for {len(duplicates)} duplicate names into subfolders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates by comparing chunks within submissions for overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"Divide text into chunks of a given size.\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def hash_chunk(chunk):\n",
    "    \"\"\"Calculate SHA256 hash of a text chunk.\"\"\"\n",
    "    return hashlib.sha256(chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "def find_related_pdfs(folder_path, chunk_size=500, threshold=0.1):\n",
    "    \"\"\"Find PDFs that share chunks of text.\"\"\"\n",
    "    chunk_hashes = {}\n",
    "    doc_chunks = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(filepath)\n",
    "            chunks = chunk_text(text, chunk_size)\n",
    "            doc_chunks[filename] = set()\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                chunk_hash = hash_chunk(chunk)\n",
    "                doc_chunks[filename].add(chunk_hash)\n",
    "                \n",
    "                if chunk_hash not in chunk_hashes:\n",
    "                    chunk_hashes[chunk_hash] = []\n",
    "                chunk_hashes[chunk_hash].append(filename)\n",
    "    \n",
    "    # Identify documents with a significant number of shared chunks\n",
    "    related_docs = []\n",
    "    for doc, hashes in doc_chunks.items():\n",
    "        shared = {}\n",
    "        for hash_val in hashes:\n",
    "            for other_doc in chunk_hashes[hash_val]:\n",
    "                if other_doc != doc:\n",
    "                    shared[other_doc] = shared.get(other_doc, 0) + 1\n",
    "        \n",
    "        # Check if the number of shared chunks exceeds the threshold\n",
    "        for other_doc, count in shared.items():\n",
    "            if count / len(hashes) > threshold:\n",
    "                related_docs.append((doc, other_doc, count))\n",
    "    \n",
    "    return related_docs\n",
    "\n",
    "def merge_groups_with_shared_documents(related_pdfs):\n",
    "    \"\"\"Merge groups that share any documents.\"\"\"\n",
    "    groups = []\n",
    "    for rel in related_pdfs:\n",
    "        found = False\n",
    "        for group in groups:\n",
    "            if rel[0] in group or rel[1] in group:\n",
    "                group.update([rel[0], rel[1]])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            groups.append(set([rel[0], rel[1]]))\n",
    "    \n",
    "    # Merge groups with any common elements\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        for i in range(len(groups)):\n",
    "            for j in range(i+1, len(groups)):\n",
    "                if groups[i].intersection(groups[j]):\n",
    "                    groups[i] = groups[i].union(groups[j])\n",
    "                    groups[j] = set()\n",
    "                    merged = True\n",
    "        groups = [group for group in groups if group]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def create_subfolders_for_duplicates(related_pdfs, folder_path, duplicates_folder=\"01. duplicates_chunked\"):\n",
    "    \"\"\"Create subfolders and move duplicates into them, merging groups with shared documents.\"\"\"\n",
    "    groups = merge_groups_with_shared_documents(related_pdfs)\n",
    "    duplicates_path = os.path.join(folder_path, duplicates_folder)\n",
    "    \n",
    "    if not os.path.exists(duplicates_path):\n",
    "        os.mkdir(duplicates_path)\n",
    "    \n",
    "    # Create subfolders and move/copy PDFs\n",
    "    for i, group in enumerate(groups):\n",
    "        subfolder_path = os.path.join(duplicates_path, f\"group_{i+1}\")\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.mkdir(subfolder_path)\n",
    "        \n",
    "        for file in group:\n",
    "            src_path = os.path.join(folder_path, file)\n",
    "            dest_path = os.path.join(subfolder_path, file)\n",
    "            shutil.copy(src_path, dest_path)  # Use shutil.move(src_path, dest_path) to move instead of copy\n",
    "\n",
    "folder_path = './static/submissions'\n",
    "related_pdfs = find_related_pdfs(folder_path)\n",
    "\n",
    "if related_pdfs:\n",
    "    print(f\"Found {len(related_pdfs)} related PDFs based on shared text chunks:\")    \n",
    "    create_subfolders_for_duplicates(related_pdfs, folder_path)\n",
    "    print(\"Duplicates have been organized into subfolders.\")\n",
    "\n",
    "else:\n",
    "    print(\"No related PDFs found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
