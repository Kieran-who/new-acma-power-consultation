{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process list of documents before analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for duplicates\n",
    "\n",
    "The record for how duplicates are handled in a spreadsheet containing all\n",
    "submissions. This is downloaded and saved to JSON format in next steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates via comparing doc hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "def hash_file(filepath):\n",
    "    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        # Read and update hash in chunks of 4K\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def find_duplicate_pdfs(folder_path):\n",
    "    \"\"\"Find duplicate PDFs in a folder.\"\"\"\n",
    "    hashes = {}\n",
    "    duplicates = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            file_hash = hash_file(filepath)\n",
    "            \n",
    "            if file_hash in hashes:\n",
    "                duplicates.append((filename, hashes[file_hash]))\n",
    "            else:\n",
    "                hashes[file_hash] = filename\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "folder_path = './static/submissions'\n",
    "duplicates = find_duplicate_pdfs(folder_path)\n",
    "\n",
    "if duplicates:\n",
    "    print(\"Found duplicates:\")\n",
    "    for dup in duplicates:\n",
    "        print(f\"{dup[0]} is a duplicate of {dup[1]}\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List documents from same author for checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    parts = filename.split('-')\n",
    "    name_parts = parts[1:]\n",
    "    name = ' '.join(name_parts).split('.')[0]\n",
    "    if name.find('anonymous') != -1:\n",
    "        name = 'anonymous'\n",
    "    return name.lower()\n",
    "\n",
    "def find_duplicate_names(folder_path):\n",
    "    name_to_files = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            name = extract_name_from_filename(filename)\n",
    "            if name != 'anonymous':\n",
    "                if name in name_to_files:\n",
    "                    name_to_files[name].append(filename)\n",
    "                else:\n",
    "                    name_to_files[name] = [filename]\n",
    "    \n",
    "    duplicates = {name: files for name, files in name_to_files.items() if len(files) > 1}\n",
    "    return duplicates\n",
    "\n",
    "def copy_files_to_subfolders(folder_path, duplicates):\n",
    "    # Create a main directory for the duplicates\n",
    "    duplicates_dir = os.path.join(folder_path, 'duplicates')\n",
    "    if not os.path.exists(duplicates_dir):\n",
    "        os.makedirs(duplicates_dir)\n",
    "    \n",
    "    for name, files in duplicates.items():\n",
    "        # Create a subfolder for each duplicate name\n",
    "        subfolder_path = os.path.join(duplicates_dir, name)\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.makedirs(subfolder_path)\n",
    "        \n",
    "        # Copy the files into their respective subfolders\n",
    "        for file in files:\n",
    "            src_path = os.path.join(folder_path, file)\n",
    "            dst_path = os.path.join(subfolder_path, file)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "folder_path = './static/submissions'\n",
    "duplicates = find_duplicate_names(folder_path)\n",
    "\n",
    "# Copy the files to subfolders\n",
    "copy_files_to_subfolders(folder_path, duplicates)\n",
    "\n",
    "print(f'Copied files for {len(duplicates)} duplicate names into subfolders.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates by comparing chunks within submissions for overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"Divide text into chunks of a given size.\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def hash_chunk(chunk):\n",
    "    \"\"\"Calculate SHA256 hash of a text chunk.\"\"\"\n",
    "    return hashlib.sha256(chunk.encode('utf-8')).hexdigest()\n",
    "\n",
    "def find_related_pdfs(folder_path, chunk_size=500, threshold=0.1):\n",
    "    \"\"\"Find PDFs that share chunks of text.\"\"\"\n",
    "    chunk_hashes = {}\n",
    "    doc_chunks = {}\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdf'):\n",
    "            filepath = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(filepath)\n",
    "            chunks = chunk_text(text, chunk_size)\n",
    "            doc_chunks[filename] = set()\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                chunk_hash = hash_chunk(chunk)\n",
    "                doc_chunks[filename].add(chunk_hash)\n",
    "                \n",
    "                if chunk_hash not in chunk_hashes:\n",
    "                    chunk_hashes[chunk_hash] = []\n",
    "                chunk_hashes[chunk_hash].append(filename)\n",
    "    \n",
    "    # Identify documents with a significant number of shared chunks\n",
    "    related_docs = []\n",
    "    for doc, hashes in doc_chunks.items():\n",
    "        shared = {}\n",
    "        for hash_val in hashes:\n",
    "            for other_doc in chunk_hashes[hash_val]:\n",
    "                if other_doc != doc:\n",
    "                    shared[other_doc] = shared.get(other_doc, 0) + 1\n",
    "        \n",
    "        # Check if the number of shared chunks exceeds the threshold\n",
    "        for other_doc, count in shared.items():\n",
    "            if count / len(hashes) > threshold:\n",
    "                related_docs.append((doc, other_doc, count))\n",
    "    \n",
    "    return related_docs\n",
    "\n",
    "def merge_groups_with_shared_documents(related_pdfs):\n",
    "    \"\"\"Merge groups that share any documents.\"\"\"\n",
    "    groups = []\n",
    "    for rel in related_pdfs:\n",
    "        found = False\n",
    "        for group in groups:\n",
    "            if rel[0] in group or rel[1] in group:\n",
    "                group.update([rel[0], rel[1]])\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            groups.append(set([rel[0], rel[1]]))\n",
    "    \n",
    "    # Merge groups with any common elements\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        for i in range(len(groups)):\n",
    "            for j in range(i+1, len(groups)):\n",
    "                if groups[i].intersection(groups[j]):\n",
    "                    groups[i] = groups[i].union(groups[j])\n",
    "                    groups[j] = set()\n",
    "                    merged = True\n",
    "        groups = [group for group in groups if group]\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def create_subfolders_for_duplicates(related_pdfs, folder_path, duplicates_folder=\"01. duplicates_chunked\"):\n",
    "    \"\"\"Create subfolders and move duplicates into them, merging groups with shared documents.\"\"\"\n",
    "    groups = merge_groups_with_shared_documents(related_pdfs)\n",
    "    duplicates_path = os.path.join(folder_path, duplicates_folder)\n",
    "    \n",
    "    if not os.path.exists(duplicates_path):\n",
    "        os.mkdir(duplicates_path)\n",
    "    \n",
    "    # Create subfolders and move/copy PDFs\n",
    "    for i, group in enumerate(groups):\n",
    "        subfolder_path = os.path.join(duplicates_path, f\"group_{i+1}\")\n",
    "        if not os.path.exists(subfolder_path):\n",
    "            os.mkdir(subfolder_path)\n",
    "        \n",
    "        for file in group:\n",
    "            src_path = os.path.join(folder_path, file)\n",
    "            dest_path = os.path.join(subfolder_path, file)\n",
    "            shutil.copy(src_path, dest_path)  # Use shutil.move(src_path, dest_path) to move instead of copy\n",
    "\n",
    "folder_path = './static/submissions'\n",
    "related_pdfs = find_related_pdfs(folder_path)\n",
    "\n",
    "if related_pdfs:\n",
    "    print(f\"Found {len(related_pdfs)} related PDFs based on shared text chunks:\")    \n",
    "    create_subfolders_for_duplicates(related_pdfs, folder_path)\n",
    "    print(\"Duplicates have been organized into subfolders.\")\n",
    "\n",
    "else:\n",
    "    print(\"No related PDFs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading spreadsheet\n",
    "\n",
    "The spreadsheet includes final decisions on how data is being labelled,\n",
    "including categories and whether any items are being removed (i.e. due to being\n",
    "duplicates etc.)\n",
    "\n",
    "This first step is to prepare the data into the correct OpenAI batch format. The\n",
    "batch will run (up to 24hrs) and we will review the results for consistency.\n",
    "Special attention will be paid in this first run to determine if the AI has\n",
    "categorised any submissions (not manually categorised) as part of our interested\n",
    "groupings. If so, we will manually review to determine if correct or not.\n",
    "\n",
    "The second step will then take all those within the special groups and run\n",
    "additional requests on them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the Excel spreadsheet into a pandas DataFrame\n",
    "df = pd.read_excel('./data/list.xlsx')\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    parts = filename.split('-')\n",
    "    name_parts = parts[1:]\n",
    "    name = ' '.join(name_parts).split('.')[0]\n",
    "    if name.find('anonymous') != -1:\n",
    "        name = 'anonymous'\n",
    "    return name.lower()\n",
    "\n",
    "formatted_data = []\n",
    "# Convert empty cells in 'Group', 'Comments', and 'Removed (Y)' columns to None\n",
    "for row in data:\n",
    "    if pd.isnull(row['Group']):\n",
    "        row['Group'] = None\n",
    "    if pd.isnull(row['Comments']):\n",
    "        row['Comments'] = None\n",
    "    if pd.isnull(row['Removed (Y)']):\n",
    "        row['Removed (Y)'] = None\n",
    "\n",
    "    file_name = row['doc'].replace('acma2023-', '').replace('.pdf', '')\n",
    "    \n",
    "    formatted_row = {\n",
    "        'uniqueId': row['UniqueID'],\n",
    "        'group': row['Group'],\n",
    "        'submitter': extract_name_from_filename(file_name),\n",
    "        'doc': file_name,\n",
    "        \"metadata\": {\n",
    "            \"groupDefinedBy\": \"human\" if row['Group'] else \"AI\",\n",
    "            \"removed\": row['Removed (Y)'],\n",
    "            \"comments\": row['Comments']            \n",
    "        }\n",
    "    }\n",
    "    formatted_data.append(formatted_row)\n",
    "\n",
    "# Save the data as a JSON file if it doesn't exist\n",
    "json_file = './data/list.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(formatted_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now have JSON of values in the below form, lets create JSONl files for batch processing\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"group\": \"string | null\",\n",
    "  \"submitter\": \"string\",\n",
    "  \"doc\": \"string\",\n",
    "  \"uniqueId\": \"string\",\n",
    "  \"metadata\": {\n",
    "    \"groupDefinedBy\": \"human or AI\",\n",
    "    \"removed\": \"string | null\",\n",
    "    \"comments\": \"string | null\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "These are the unique groups:\n",
    "\n",
    "`defined_group = ['academic', 'civil', '???', 'government', 'industry', 'news', 'platform', 'political']`\n",
    "\n",
    "This still contains all entries, even ones that are removed. The next step will\n",
    "ignore removed entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run of batch request, preliminary function across all submissions\n",
    "\n",
    "After this, we will review how the AI has grouped each submission that has not\n",
    "already been manually classified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the prompt for each individual request\n",
    "def prompt_formatted(submission_string: str, submission_author: str) -> str:    \n",
    "    # Read the first file and set a string variable\n",
    "    with open('prompt.txt', 'r') as file:\n",
    "        prompt = file.read()\n",
    "        \n",
    "    with open('prompt_issues.md', 'r') as file:\n",
    "        issues = file.read()\n",
    "\n",
    "    with open('prompt_guidance_note.md', 'r') as file:\n",
    "        guidance_note = file.read()\n",
    "\n",
    "    with open('prompt_fact_sheet.md', 'r') as file:\n",
    "        fact_sheet = file.read()\n",
    "\n",
    "    prompt = prompt.replace('|issues|', issues)\n",
    "    prompt = prompt.replace('|guidance_note|', guidance_note)\n",
    "    prompt = prompt.replace('|fact_sheet|', fact_sheet)\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION START *****************************************\\n\\n\"\n",
    "\n",
    "    prompt += f\"Submission from: {submission_author}\\n\\n\"\n",
    "    \n",
    "    prompt += submission_string\n",
    "\n",
    "    prompt += \"\\n\\n***************************************** SUBMISSION END *****************************************\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "    if (category == 'civil' or category == 'academic'):\n",
    "        with open('function_civil_society.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    if (category == 'platform'):\n",
    "        with open('function_digital.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "    if (category == 'news'):\n",
    "        with open('function_news.json', 'r') as f:\n",
    "            return json.load(f) \n",
    "    if (category == 'government' or category == 'political'):\n",
    "        with open('function_pol.json', 'r') as f:\n",
    "            return json.load(f) \n",
    "\n",
    "def get_function():\n",
    "    with open('function.json', 'r') as f:\n",
    "        function = json.load(f)\n",
    "    return function \n",
    "\n",
    "with open('./data/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "# md_file_location = './data/files/md_files'\n",
    "md_file_location = './data/files/converted'\n",
    "\n",
    "file_counter = 0\n",
    "jsonl_file = f\"./data/all/jsonl_{file_counter}.jsonl\"\n",
    "\n",
    "for i in list:\n",
    "    if i[\"metadata\"][\"removed\"] == \"Y\":\n",
    "        continue\n",
    "    try:\n",
    "        # md_file_path = f\"{md_file_location}/{i[\"doc\"]}/{i[\"doc\"]}.md\"\n",
    "        md_file_path = f\"{md_file_location}/{i[\"doc\"]}.md\"\n",
    "        with open(md_file_path, 'r') as file:\n",
    "            submission = file.read()\n",
    "        sub_author = i[\"submitter\"]\n",
    "        prompt = prompt_formatted(submission, sub_author)\n",
    "        function = get_function()\n",
    "        ldata = {\"custom_id\": i[\"uniqueId\"], \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-2024-05-13\", \"messages\": [{\"role\": \"user\", \"content\": prompt}],\"max_tokens\": 4096,\"temperature\": 0, \"tools\":[function], \"tool_choice\":{ 'type': 'function', 'function': { 'name': 'submission_eval' } }}}        \n",
    "        \n",
    "        if os.path.exists(jsonl_file) and os.path.getsize(jsonl_file) >= 85 * 1024 * 1024:  # 90MB\n",
    "            file_counter += 1\n",
    "            jsonl_file = f\"./data/all/jsonl_{file_counter}.jsonl\"\n",
    "        \n",
    "        with open(jsonl_file, 'a') as f:\n",
    "            json.dump(ldata, f)\n",
    "            f.write('\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We should now have a folder with all the prepared batch calls\n",
    "\n",
    "We will upload each of these files to OpenAI and then call the batch function\n",
    "for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "jsonl_dir = './data/all'\n",
    "\n",
    "jsonl_files = [f for f in os.listdir(jsonl_dir) if os.path.isfile(os.path.join(jsonl_dir, f)) and f.endswith('.jsonl')]\n",
    "\n",
    "file_ids = []\n",
    "\n",
    "for file in jsonl_files:\n",
    "    file_object = client.files.create(\n",
    "        file=open(f\"{jsonl_dir}/{file}\", \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    file_ids.append(file_object.id)\n",
    "\n",
    "# We have now uploaded all the files and have their IDs, lets create a batch job for each\n",
    "\n",
    "batch_ids = []\n",
    "\n",
    "for file_id in file_ids:\n",
    "    job = client.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "          )\n",
    "    print(job)\n",
    "    batch_ids.append(job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The batch processes should now be completing, they will take up to 24hrs\n",
    "\n",
    "We can run the following cell to check on process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_KEY'),max_retries=3)\n",
    "\n",
    "batch_jobs = client.batches.list()\n",
    "\n",
    "print(f'Total number of batch jobs: {len(batch_jobs.data)}')\n",
    "for batch in batch_jobs.data:\n",
    "    print(batch.id, batch.status, batch.request_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
