{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook creates chunks of each submission, grabs an embedding and uploads to db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from db.chunks import ChunkManager\n",
    "from db.db_instance import DBClient\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "client = DBClient()\n",
    "db = ChunkManager()\n",
    "\n",
    "def chunk_text(md_text):\n",
    "    init_split = md_text.replace(\":\\n\\n\", \":\\n\").replace(\"\\n\\n-\", \"\\n-\").split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    cur_header = \"\"\n",
    "    previous_str = \"\"\n",
    "    for i in init_split:\n",
    "        if (i.startswith(\"#\")):\n",
    "            # if there is a new header, and previous_str is not empty, lets package up previous string and append to chunks\n",
    "            if previous_str:\n",
    "                chunks.append(f\"{cur_header}\\n{previous_str.strip()}\")\n",
    "                previous_str = \"\"\n",
    "            cur_header = i\n",
    "            continue\n",
    "        if len(f\"{previous_str}\\n{i.strip()}\") > 200:\n",
    "            if \"png](\" in i:\n",
    "                second_split = i.split(\"\\n\")                \n",
    "                second_clean = \"\"\n",
    "                for j in second_split:\n",
    "                    if len(j.strip()) > 2 and \"png]\" not in j:\n",
    "                        second_clean += j\n",
    "                if len(f\"{previous_str}\\n{second_clean.strip()}\") > 200:\n",
    "                    fixed_string = re.sub(r'\\r?\\n\\s*', ' ', second_clean).strip()\n",
    "                    if len(fixed_string) > 6000:\n",
    "                        # If the fixed string is too long, lets split it up into 3000 character chunks\n",
    "                        split_fixed = [fixed_string[i:i+3000] for i in range(0, len(fixed_string), 3000)]\n",
    "                        for split in split_fixed:\n",
    "                            append_str = cur_header\n",
    "                            if previous_str:\n",
    "                                append_str += f\"\\n{previous_str.strip()}\"\n",
    "                            append_str += f\"\\n{split}\"\n",
    "                            chunks.append(append_str.strip())\n",
    "                            previous_str = \"\"\n",
    "                        continue\n",
    "                    append_str = cur_header\n",
    "                    if previous_str:\n",
    "                        append_str += f\"\\n{previous_str.strip()}\"\n",
    "                    append_str += f\"\\n{fixed_string}\"                    \n",
    "                    chunks.append(append_str.strip())                    \n",
    "                    previous_str = \"\"\n",
    "                else:\n",
    "                    previous_str += f\"{second_clean.strip()}\\n\"            \n",
    "            else:                \n",
    "                fixed_string = re.sub(r'\\r?\\n\\s*', ' ', i)\n",
    "                if len(fixed_string) > 6000:\n",
    "                    # If the fixed string is too long, lets split it up into 3000 character chunks\n",
    "                    split_fixed = [fixed_string[i:i+3000] for i in range(0, len(fixed_string), 3000)]\n",
    "                    for split in split_fixed:\n",
    "                        append_str = cur_header\n",
    "                        if previous_str:\n",
    "                            append_str += f\"\\n{previous_str.strip()}\"\n",
    "                        append_str += f\"\\n{split}\"\n",
    "                        chunks.append(append_str.strip())\n",
    "                        previous_str = \"\"\n",
    "                    continue\n",
    "                append_str = cur_header\n",
    "                if previous_str:\n",
    "                    append_str += f\"\\n{previous_str.strip()}\"\n",
    "                append_str += f\"\\n{fixed_string}\"\n",
    "                chunks.append(append_str.strip())                \n",
    "                previous_str = \"\"   \n",
    "        else:\n",
    "            # Still need to remove images from the text\n",
    "            if \"png](\" in i.lower():\n",
    "                second_split = i.split(\"\\n\")\n",
    "                second_clean = \"\"\n",
    "                for j in second_split:\n",
    "                    if len(j.strip()) > 2 and \"png]\" not in j.lower():\n",
    "                        second_clean += j\n",
    "                fixed_string = re.sub(r'\\r?\\n\\s*', ' ', second_clean).strip()\n",
    "                previous_str += f\"{second_clean.strip()}\\n\"                \n",
    "            else:\n",
    "                fixed_string = re.sub(r'\\r?\\n\\s*', ' ', i).strip()\n",
    "                previous_str += f\"{fixed_string.strip()}\\n\"\n",
    "    # If there is still a previous_str, lets append it as the last chunk\n",
    "    if previous_str:        \n",
    "        chunks.append(f\"{cur_header}\\n{previous_str.strip()}\")\n",
    "    return chunks\n",
    "\n",
    "# Data to process\n",
    "with open('./data/step1/list.json', 'r') as f:\n",
    "    list = json.load(f)\n",
    "\n",
    "data = list[\"data\"]\n",
    "\n",
    "# this function will take a doc_id and return the file path for the md file\n",
    "def get_file_path(doc_id, folder_path = './data/files'):    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.split('-')[0] == doc_id:\n",
    "            return os.path.join(folder_path, file_name)\n",
    "\n",
    "error_items = []\n",
    "\n",
    "# To show number of chunks being added\n",
    "up_counter_bar = tqdm(desc=\"Creating items\", unit=\"item\")\n",
    "\n",
    "def chunk_submission(item, pbar):\n",
    "    try: \n",
    "        md_file_path = get_file_path(item.get(\"uniqueId\"))       \n",
    "        with open(md_file_path, 'r') as file:\n",
    "            submission = file.read()\n",
    "        chunks = chunk_text(submission)\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            chunk_obj = {\n",
    "                \"submission_uniqueId\": item[\"uniqueId\"],\n",
    "                \"chunk_text\": chunk,\n",
    "                \"submitter\": item['submitter'],\n",
    "                \"group\": item['group'],\n",
    "                \"support\": item.get('step_1', {}).get('support', {}).get('support', None),\n",
    "                \"regulation_type\": item.get('step_1', {}).get('regulation', {}).get('regulation_type', None),\n",
    "                \"regulator_trust\": item.get('step_1', {}).get('regulator_trust', {}).get('regulator_trust', None),\n",
    "                \"chunk_index\": idx\n",
    "            }        \n",
    "            db.new_chunk(chunk_obj, chunk)\n",
    "            up_counter_bar.update(1)\n",
    "        pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        error_items.append(item)\n",
    "        pbar.update(1)\n",
    "        return\n",
    "\n",
    "with tqdm(total=len(data), desc=\"Overall Progress\") as main_pbar:\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = {executor.submit(chunk_submission, item, main_pbar): item for item in data}\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "up_counter_bar.close()\n",
    "\n",
    "# Data to process\n",
    "with open('error_adding_chunks', 'w') as wile:\n",
    "    list = json.dump(error_items, wile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
